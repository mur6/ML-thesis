
HANDS play a central role in the interaction between humans and the environment, from physical contact and grasping to daily communications via hand gesture. Learning 3D hand reconstruction is the preliminary step for many computer vision applications such as augmented reality [1], sign language translation [2], [3], and human- computer interaction [4], [5], [6]. However, due to diverse hand configurations and interaction with the environment, 3D hand reconstruction remains a challenging problem, especially when the task relies on monocular data as input.
HANDSは、物理的な接触や握り方から、手のジェスチャーによる日常のコミュニケーションまで、人間と環境の間の相互作用において中心的な役割を果たします。 3D手の再構成の学習は、拡張現実[1]、手話翻訳[2]、[3]、人間とコンピューターの相互作用[4]、[5]、[6]などの多くのコンピュータービジョンアプリケーションの準備段階です。ただし、手の構成と環境との相互作用が多様であるため、特にタスクが入力として単眼データに依存している場合、3D手の再構築は依然として困難な問題です。

Compared with multi-view images [7], [8], [9], [10] and depth maps [11], [12], [13], [14], [15], the monocular hand images are more common in practice. In recent years, we have witnessed many efforts in recovering 3D shapes of human hands from single-view RGB images. For example, [16], [17], [18], [19], [20] were proposed to predict 3D hand pose from an RGB image. However, they only represent the 3D hand through sparse joints, and ignore the 3D shape information, which are required for some applications such as grasping objects with virtual hands [4]. To better capture the surface information of the hand, previous studies predict the triangle mesh either via regressing per-vertex coordinate [21], [22] or by deforming a parametric hand model [23], [24].
マルチビュー画像[7]、[8]、[9]、[10]および深度マップ[11]、[12]、[13]、[14]、[15]と比較すると、単眼の手の画像はより多くなります。実際には一般的です。近年、シングルビューRGB画像から人間の手の3D形状を復元するための多くの努力を目の当たりにしてきました。たとえば、[16]、[17]、[18]、[19]、[20]は、RGB画像から3D手のポーズを予測するために提案されました。ただし、これらはまばらな関節を介した3Dハンドのみを表し、仮想ハンドでオブジェクトをつかむなどの一部のアプリケーションに必要な3D形状情報を無視します[4]。手の表面情報をより適切にキャプチャするために、以前の研究では、頂点ごとの座標を回帰することによって[21]、[22]、またはパラメトリックハンドモデルを変形することによって[23]、[24]三角形メッシュを予測しています。



Outputting such high-dimensional representation from 2D input is challenging for neural networks to learn.
2D入力からこのような高次元表現を出力することは、ニューラルネットワークが学習するのが困難です。

As a result, the training process relies heavily on 3D hand annotations such as dense hand scans, model-fitted parametric hand mesh, or human-annotated 3D joints.
その結果、トレーニングプロセスは、密なハンドスキャン、モデルに適合したパラメトリックハンドメッシュ、または人間が注釈を付けた3Dジョイントなどの3D手の注釈に大きく依存しています。

Besides, the hand texture is important in some applications, such as vivid hands reconstruction in immersive virtual reality.
But only recent work try to explore parametric texture estimation in a learning-based hand recovery system [25],
while most previous work of 3D hand reconstruction do not consider texture modeling [25].
さらに、手の質感は、没入型バーチャルリアリティでの鮮やかな手の再構成など、一部のアプリケーションでは重要です。
しかし、最近の研究だけが、学習ベースの手の回復システムでパラメトリックテクスチャ推定を探求しようとしています[25]、
一方、3D手の再構成に関するこれまでのほとんどの作業では、テクスチャモデリングは考慮されていません[25]。

One of our key observations is that the 2D cues in the hand image are quite informative to reconstruct the 3D hand model in the real world. The 2D hand keypoints contain rich structural information, and the 2D image contains abundant texture and shape information. Both are important for re- ducing the use of expensive 3D annotations but have not been fully investigated. Leveraging these cues, we could directly use 2D annotations and the input image to learn the geometry and texture representations without relying on 3D annotations [19]. However, it is still labor-consuming to annotate 2D hand keypoints per image. To completely save the manual annotation, we propose to extract 2D hand keypoints as well as geometric representations from the unlabeled hand image to help the shape reconstruction and use the texture information contained in the input image to help the texture modeling.
私たちの重要な観察の1つは、手の画像の2Dキューは、現実の世界で3D手のモデルを再構築するのに非常に有益であるということです。 2Dハンドキーポイントには豊富な構造情報が含まれ、2D画像には豊富なテクスチャおよび形状情報が含まれています。どちらも高価な3D注釈の使用を減らすために重要ですが、十分に調査されていません。これらの手がかりを活用して、2D注釈と入力画像を直接使用して、3D注釈に依存することなくジオメトリとテクスチャ表現を学習することができます[19]。ただし、画像ごとに2Dハンドキーポイントに注釈を付けるのは依然として手間がかかります。手動注釈を完全に保存するために、ラベルのない手の画像から2D手のキーポイントと幾何学的表現を抽出して形状の再構築を支援し、入力画像に含まれるテクスチャ情報を使用してテクスチャモデリングを支援することを提案します。

Additionally, video sequences contain rich hand motion and more comprehensive appearance information. Usually, a frame-to-frame fully-supervised hand reconstruction model does not take these information into serious con- sideration since 3D annotations already provide a strong supervision. As a result, it is more difficult for a frame- to-frame model to produce consistent results from video frames compared to sequence-to-sequence models, since no temporal information is utilized. Thereby, we propose to penalize the inconsistency of the output hand reconstruc- tions from consecutive observations of the same hand. In this way, motion prior in video is distilled in the frame- to-frame model to help reconstruct more accurate hand for every single frame. Notably, the constraints on the sequence output are also employed in a self-supervised manner.
さらに、ビデオシーケンスには、豊富な手の動きとより包括的な外観情報が含まれています。通常、フレーム間で完全に監視された手の再構成モデ​​ルでは、3D注釈がすでに強力な監視を提供しているため、これらの情報を真剣に検討することはありません。その結果、時間情報が利用されないため、フレーム間モデルがシーケンス間モデルと比較してビデオフレームから一貫した結果を生成することはより困難になります。それにより、同じ手の連続観察からの出力手の再構成の不一致にペナルティを課すことを提案します。このように、ビデオの事前の動きはフレーム間モデルで抽出され、すべての単一フレームに対してより正確な手を再構築するのに役立ちます。特に、シーケンス出力の制約も自己監視方式で採用されています。

Driven by the above observations, this work aims to train an accurate 3D hand reconstruction network using only the supervision signals obtained from the input images or video sequences while eliminating manual annotations of the training images. To this end, we use an off-the- shelf 2D keypoint detector [26] to generate some noisy 2D keypoints, and supervise the hand reconstruction by these noisy detected 2D keypoints and the input image. Further, we leverage the self-supervision signal embedded in the video sequence to help the network produce more accurate and and temporally more coherent hand reconstructions. To learn in a self-supervised manner, there are several issues to be addressed. First, how to efficiently use joint-wise 2D keypoints to supervise the ill-posed monocular 3D hand reconstruction? Second, how to handle noise in the 2D detection output since our setting is without utilizing any ground truth annotation? Third, is it possible to make use of the continuous information contained in video sequences to encourage smoothness and consistency of reconstructed hands in a frame-to-frame model?
上記の観察に基づいて、この作業は、トレーニング画像の手動注釈を排除しながら、入力画像またはビデオシーケンスから取得された監視信号のみを使用して正確な3D手の再構成ネットワークをトレーニングすることを目的としています。この目的のために、市販の2Dキーポイント検出器[26]を使用して、ノイズの多い2Dキーポイントを生成し、これらのノイズの多い2Dキーポイントと入力画像による手の再構成を監視します。さらに、ビデオシーケンスに埋め込まれた自己監視信号を活用して、ネットワークがより正確で時間的にコヒーレントな手の再構成を生成できるようにします。自己管理の方法で学ぶために、取り組むべきいくつかの問題があります。まず、ジョイントワイズ2Dキーポイントを効率的に使用して、不適切な単眼3D手の再構成を監視する方法を教えてください。次に、グラウンドトゥルースアノテーションを使用しない設定であるため、2D検出出力のノイズを処理するにはどうすればよいですか？第三に、ビデオシーケンスに含まれる連続情報を利用して、フレーム間モデルで再構築された手の滑らかさと一貫性を促進することは可能ですか？

To address the first issue, a model-based autoencoder is learned to estimate 3D joints and shape, where the output 3D joints are projected into 2D image space and forced to align with the detected keypoints during training. However, if we only align keypoints in image space, invalid hand pose often occurs. This may be caused by an invalid 3D hand configure which is still compatible with the projected 2D keypoints. Furthermore, 2D keypoints cannot reduce the scale ambiguity of the predicted 3D hand. Thus, we propose to learn a series of priors embedded in the model-based hand representations to help the neural network output hand with a reasonable pose and size.
最初の問題に対処するために、モデルベースのオートエンコーダーを学習して3Dジョイントと形状を推定します。ここで、出力3Dジョイントは2D画像空間に投影され、トレーニング中に検出されたキーポイントと位置合わせされます。ただし、画像空間でキーポイントのみを配置すると、無効な手のポーズが発生することがよくあります。これは、投影された2Dキーポイントとまだ互換性のある無効な3Dハンド構成が原因である可能性があります。さらに、2Dキーポイントは、予測された3D手のスケールのあいまいさを減らすことはできません。したがって、モデルベースの手の表現に埋め込まれた一連の事前情報を学習して、ニューラルネットワークが適切なポーズとサイズで手を出力できるようにすることを提案します。

To address the second issue, a trainable 2D keypoint estimator and a novel 2D-3D consistency loss are proposed. The 2D keypoint estimator outputs joint-wise 2D keypoints and the 2D-3D consistency loss links the 2D keypoint esti- mator and the 3D reconstruction network to make the two mutually beneficial to each other during the training. In addition, we find that the detection accuracy of different samples varies greatly, thus we propose to distinguish each detection item to weigh its supervision strength accordingly.
2番目の問題に対処するために、トレーニング可能な2Dキーポイント推定器と新しい2D-3D整合性損失が提案されます。 2Dキーポイント推定器はジョイントワイズ2Dキーポイントを出力し、2D-3D整合性損失は2Dキーポイント推定器と3D再構成ネットワークをリンクして、トレーニング中に2つを相互に有益にします。また、サンプルごとに検出精度が大きく異なることがわかったため、各検出項目を区別し、それに応じて監視強度を比較検討することを提案します。

To address the third issue, we decompose the hand mo- tion into the joint rotations and ensure smooth rotations of hand joints between frames by conforming to a quaternion- based representation. Furthermore, a novel quaternion loss function is proposed to allow all possible rotation speeds. Besides motion consistency, hand appearance is another main concern. A T&S (texture and shape) consistency loss function is introduced to regularize the coherence of the output hand texture and shape.
3番目の問題に対処するために、手の動きを関節の回転に分解し、クォータニオンベースの表現に準拠することにより、フレーム間の手の関節のスムーズな回転を確保します。さらに、すべての可能な回転速度を可能にするために、新しいクォータニオン損失関数が提案されています。動きの一貫性に加えて、手の外観は別の主な関心事です。 T＆S（テクスチャと形状）の一貫性損失関数が導入され、出力された手のテクスチャと形状の一貫性が正規化されます

In brief, we present a self-supervised 3D hand recon- struction (S2 HAND) model and its advanced S2 HAND(V ). The models enable us to train neural networks that can predict 3D pose, shape, texture and camera viewpoint from images without any ground truth annotation of training images, except that we use the outputs from a 2D keypoint detector (see Fig. 1). Notably, S2HAND(V) is able to extract informative supervision from unannotated videos to help learn a better frame-to-frame model. In order to achieve this, S2HAND(V) inputs the sequential data to multiple weight- shared S2HAND models and employs proposed constraints on the sequential output at the training stage.
簡単に説明すると、自己監視型3D手再構成（S2 HAND）モデルとその高度なS2 HAND（V）を紹介します。モデルを使用すると、2Dキーポイント検出器からの出力を使用することを除いて、トレーニング画像のグラウンドトゥルースアノテーションなしで、画像から3Dポーズ、形状、テクスチャ、カメラの視点を予測できるニューラルネットワークをトレーニングできます（図1を参照）。特に、S2HAND（V）は、注釈のないビデオから有益な監督を抽出して、より優れたフレーム間モデルの学習に役立てることができます。これを実現するために、S2HAND（V）は、複数の重み共有S2HANDモデルにシーケンシャルデータを入力し、トレーニング段階でシーケンシャル出力に提案された制約を採用します。

The advantage of our proposed methods are summarized as follows:
提案する方法の利点は次のように要約されます:

• We present the first self-supervised 3D hand reconstruction models, which accurately outputs 3D joints, mesh, and texture from a single image, without using any annotated training data.
•注釈付きのトレーニングデータを使用せずに、単一の画像から3D関節、メッシュ、テクスチャを正確に出力する、最初の自己監視型3D手の再構成モデ​​ルを紹介します。

• We exploit an additional trainable 2D keypoint estimator to boost the 3D reconstruction through a mutual improvement manner, in which a novel 2D-3D consistency loss is constructed.
•追加のトレーニング可能な2Dキーポイント推定器を利用して、相互改善の方法で3D再構成を強化します。この方法では、新しい2D-3D整合性損失が構築されます。

• We introduce a hand texture estimation module to learn vivid hand texture via self-supervision.
•手触り推定モジュールを導入し、自己監視により鮮やかな手触りを学習します。

• We benchmark self-supervised 3D hand reconstruc- tion on some currently challenging datasets, where our self-supervised method achieves comparable performance to previous fully-supervised methods.
•現在困難なデータセットで自己監視3D手の再構築のベンチマークを行います。この場合、自己監視方式は、以前の完全監視方式と同等のパフォーマンスを実現します。

This work is an extension of our conference paper [27]. The new contributions include:
この作品は私たちの会議論文[27]の延長です。新しい貢献は次のとおりです:

• We extend our S2 HAND model to the S2 HAND(V) model, which further exploits the self-supervision signals embedded in video sequences without adopting additional temporal modules. The improvement in accuracy and smoothness is 3.5% and 3.1% respectively, measured in MPJPE and ACC.
•S2HANDモデルをS2HAND（V）モデルに拡張します。これにより、追加の時間モジュールを採用することなく、ビデオシーケンスに埋め込まれた自己監視信号をさらに活用できます。 MPJPEとACCで測定した場合、精度と滑らかさの向上はそれぞれ3.5％と3.1％です。

• We present a quaternion loss function, which is based on an explored motion-aware joints rotation representation, to help learn smooth hand motion. Experiments demonstrate its significant advantage over similar methods in both accuracy and smoothness.
•スムーズな手の動きを学習するために、探索されたモーション認識関節の回転表現に基づくクォータニオン損失関数を提示します。実験は、精度と滑らかさの両方において、同様の方法に比べてその重要な利点を示しています。

• We propose a texture and shape consistency regularization term to encourage coherent shape and texture reconstruction.
•コヒーレントな形状とテクスチャの再構築を促進するために、テクスチャと形状の一貫性の正規化用語を提案します。

• We illustrate that utilizing extra in-the-wild unlabeled training data can further boost the performance of our model.
•追加の野生のラベルのないトレーニングデータを利用することで、モデルのパフォーマンスをさらに向上させることができることを示します。

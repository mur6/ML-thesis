# MultiMAE: Multi-modal Multi-task Masked Autoencoders

## Abstract
We propose a pre-training strategy called Multi-modal Multi-task Masked Autoencoders (MultiMAE).
マルチモーダルマルチタスクマスクオートエンコーダ（MultiMAE）と呼ばれる事前トレーニング戦略を提案します。

It differs from standard Masked Autoencoding in two key aspects:
標準のマスクされた自動エンコーディングとは、2つの重要な点で異なります。

I) it can optionally accept additional modalities of information in the input besides the RGB image (hence "multi-modal"), and II) its training objective accordingly includes predicting multiple outputs besides the RGB image (hence "multi-task").
I）RGB画像以外の入力で追加の情報モダリティをオプションで受け入れることができます（したがって「マルチモーダル」）。II）トレーニングの目的には、それに応じて、 RGB画像（したがって「マルチタスク」）。

We make use of masking (across image patches and input modalities) to make training MultiMAE tractable as well as to ensure cross-modality predictive coding is indeed learned by the network.
マスキング（画像パッチと入力モダリティ全体）を利用して、トレーニングMultiMAEを扱いやすくし、クロスモダリティ予測コーディングが実際にネットワークによって学習されるようにします。

We show this pre-training strategy leads to a flexible, simple, and efficient framework with improved transfer results to downstream tasks.
この事前トレーニング戦略が、ダウンストリームタスクへの転送結果を改善した、柔軟でシンプルかつ効率的なフレームワークにつながることを示します。

In particular, the same exact pre-trained network can be flexibly used when additional information besides RGB images is available or when no information other than RGB is available - in all configurations yielding competitive to or significantly better results than the baselines. To avoid needing training datasets with multiple modalities and tasks, we train MultiMAE entirely using pseudo labeling, which makes the framework widely applicable to any RGB dataset.
特に、RGB画像以外の追加情報が利用可能な場合、またはRGB以外の情報が利用できない場合、同じ正確な事前トレーニング済みネットワークを柔軟に使用できます。すべての構成で、ベースラインと競合するか、ベースラインよりも大幅に優れた結果が得られます。複数のモダリティとタスクを備えたデータセットのトレーニングが不要になるように、疑似ラベリングを使用してMultiMAEを完全にトレーニングします。これにより、フレームワークがあらゆるRGBデータセットに広く適用できるようになります。

The experiments are performed on multiple transfer tasks (image classification, semantic segmentation, depth estimation) and datasets (ImageNet, ADE20K, Taskonomy, Hypersim, NYUv2). The results show an intriguingly impressive capability by the model in cross-modal/task predictive coding and transfer.
実験は、複数の転送タスク（画像分類、セマンティックセグメンテーション、深度推定）およびデータセット（ImageNet、ADE20K、Taskonomy、Hypersim、NYUv2）で実行されます。結果は、クロスモーダル/タスク予測コーディングおよび転送におけるモデルによる興味深い機能を示しています。

Code, pre-trained models, and interactive visualizations are available at https://multimae.epfl.ch.

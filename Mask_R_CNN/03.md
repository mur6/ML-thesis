# Mask R-CNN
## 3. Mask R-CNN
Mask R-CNN is conceptually simple:
マスクR-CNNは概念的に単純です。

Faster R-CNN has two outputs for each candidate object, a class label and a bounding-box offset;
より高速なR-CNNには、候補オブジェクトごとに2つの出力、クラスラベルとバウンディングボックスオフセットがあります。

to this we add a third branch that outputs the object mask.
これに、オブジェクトマスクを出力する3番目のブランチを追加します。

Mask R-CNN is thus a natural and in- tuitive idea. But the additional mask output is distinct from the class and box outputs, requiring extraction of much finer spatial layout of an object.
したがって、マスクR-CNNは自然で直感的なアイデアです。ただし、追加のマスク出力はクラスおよびボックス出力とは異なり、オブジェクトのより細かい空間レイアウトを抽出する必要があります。

Next, we introduce the key elements of Mask R-CNN, including pixel-to-pixel alignment, which is the main missing piece of Fast/Faster R-CNN.
次に、Fast / Faster R-CNNの主要な欠落部分であるピクセル間アライメントを含む、MaskR-CNNの主要な要素を紹介します。

Faster R-CNN: We begin by briefly reviewing the Faster R-CNN detector [36].
Faster R-CNN：まず、FasterR-CNN検出器を簡単に確認します[36]。

Faster R-CNN consists of two stages.
FasterR-CNNは2つのステージで構成されています。

The first stage, called a Region Proposal Network (RPN), proposes candidate object bounding boxes.
リージョンプロポーザルネットワーク（RPN）と呼ばれる最初のステージでは、候補オブジェクトの境界ボックスを提案します。

The second stage, which is in essence Fast R-CNN [12], extracts features using RoIPool from each candidate box and performs classification and bounding-box regression.
本質的にFastR-CNN[12]である、第2段階では、各候補ボックスからRoIPoolを使用して特徴を抽出し、分類とバウンディングボックス回帰を実行します。

The features used by both stages can be shared for faster inference. We refer readers to [21] for latest, comprehensive comparisons between Faster R-CNN and other frameworks.
両方のステージで使用される機能を共有して、推論を高速化できます。 Faster R-CNNと他のフレームワークとの最新の包括的な比較については、読者に[21]を参照してください。

Mask R-CNN: Mask R-CNN adopts the same two-stage procedure, with an identical first stage (which is RPN).
マスクR-CNN：マスクR-CNNは、同じ2段階の手順を採用し、最初の段階は同じです（RPN）。

In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI.
第2段階では、クラスとボックスオフセットの予測と並行して、MaskR-CNNは各RoIのバイナリマスクも出力します。

This is in contrast to most recent systems, where clas- sification depends on mask predictions (e.g. [33, 10, 26]).
これは、分類がマスクの予測に依存する最新のシステムとは対照的です（[33、10、26]など）。

Our approach follows the spirit of Fast R-CNN [12] that applies bounding-box classification and regression in par- allel (which turned out to largely simplify the multi-stage pipeline of original R-CNN [13]).
私たちのアプローチは、バウンディングボックスの分類と回帰を並列に適用するFast R-CNN [12]の精神に従います（これにより、元のR-CNN [13]の多段階パイプラインが大幅に簡素化されました）。

Formally, during training, we define a multi-task loss on each sampled RoI as L = Lcls + Lbox + Lmask.
正式には、トレーニング中に、サンプリングされた各RoIのマルチタスク損失をL = Lcls + Lbox+Lmaskとして定義します。

The classification loss Lcls and bounding-box loss Lbox are identical as those defined in [12].
分類損失Lclsとバウンディングボックス損失Lboxは、[12]で定義されているものと同じです。

The mask branch has a Km2- dimensional output for each RoI, which encodes K binary masks of resolution m × m, one for each of the K classes.
マスクブランチには、RoIごとにKm2次元の出力があり、Kクラスごとに1つずつ、解像度m×mのK個のバイナリマスクをエンコードします。

To this we apply a per-pixel sigmoid, and define Lmask as the average binary cross-entropy loss.
これに、ピクセルごとのシグモイドを適用し、Lmaskを平均バイナリクロスエントロピー損失として定義します。

For an RoI associated with ground-truth class k, Lmask is only defined on the k-th mask (other mask outputs do not contribute to the loss).
グラウンドトゥルースクラスkに関連付けられたRoIの場合、Lmaskはk番目のマスクでのみ定義されます（他のマスク出力は損失に寄与しません）。

Our definition of Lmask allows the network to generate masks for every class without competition among classes;
Lmaskの定義により、ネットワークはクラス間で競合することなく、すべてのクラスのマスクを生成できます。

we rely on the dedicated classification branch to predict the class label used to select the output mask. This decouples mask and class prediction. This is different from common practice when applying FCNs [30] to semantic segmenta- tion, which typically uses a per-pixel softmax and a multino- mial cross-entropy loss. In that case, masks across classes compete; in our case, with a per-pixel sigmoid and a binary loss, they do not. We show by experiments that this formu- lation is key for good instance segmentation results.
出力マスクの選択に使用されるクラスラベルを予測するために、専用の分類ブランチに依存しています。これにより、マスクとクラスの予測が分離されます。これは、FCN [30]をセマンティックセグメンテーションに適用する場合の一般的な方法とは異なります。セマンティックセグメンテーションは、通常、ピクセルごとのソフトマックスと多項クロスエントロピー損失を使用します。その場合、クラス間のマスクが競合します。私たちの場合、ピクセルごとのS状結腸と2値の損失があるため、そうではありません。この定式化が優れたインスタンスセグメンテーション結果の鍵であることを実験によって示します。

Mask Representation: A mask encodes an input object’s spatial layout.
マスク表現：マスクは、入力オブジェクトの空間レイアウトをエンコードします。

Thus, unlike class labels or box offsets that are inevitably collapsed into short output vectors by fully-connected (fc) layers, extracting the spatial structure of masks can be addressed naturally by the pixel-to-pixel correspondence provided by convolutions.
したがって、完全に接続された（fc）レイヤーによって必然的に短い出力ベクトルに折りたたまれるクラスラベルやボックスオフセットとは異なり、マスクの空間構造の抽出は、畳み込みによって提供されるピクセル間の対応によって自然に対処できます。

Specifically, we predict an m × m mask from each RoI using an FCN [30]. This allows each layer in the mask branch to maintain the explicit m × m object spatial layout without collapsing it into a vector representation that lacks spatial dimensions.
具体的には、FCNを使用して各RoIからm×mマスクを予測します[30]。これにより、マスクブランチの各レイヤーは、空間次元のないベクトル表現に折りたたむことなく、明示的なm×mオブジェクトの空間レイアウトを維持できます。

Unlike previous methods that re- sort to fc layers for mask prediction [33, 34, 10], our fully convolutional representation requires fewer parameters, and is more accurate as demonstrated by experiments.
マスク予測のためにfcレイヤーに再ソートする以前の方法[33、34、10]とは異なり、完全畳み込み表現では必要なパラメーターが少なく、実験で示されているように正確です。

This pixel-to-pixel behavior requires our RoI features, which themselves are small feature maps, to be well aligned to faithfully preserve the explicit per-pixel spatial correspondence.
このピクセル間の動作では、RoI機能（それ自体は小さな機能マップ）を適切に調整して、ピクセルごとの明示的な空間対応を忠実に維持する必要があります。

This motivated us to develop the following RoIAlign layer that plays a key role in mask prediction.
これにより、マスク予測で重要な役割を果たす次のRoIAlignレイヤーを開発することになりました。

RoIAlign: RoIPool [12] is a standard operation for extract- ing a small feature map (e.g., 7×7) from each RoI. RoIPool first quantizes a floating-number RoI to the discrete granu- larity of the feature map, this quantized RoI is then subdi- vided into spatial bins which are themselves quantized, and finally feature values covered by each bin are aggregated (usually by max pooling). Quantization is performed, e.g., on a continuous coordinate x by computing [x/16], where 16 is a feature map stride and [·] is rounding; likewise, quan- tization is performed when dividing into bins (e.g., 7×7). These quantizations introduce misalignments between the RoI and the extracted features. While this may not impact classification, which is robust to small translations, it has a large negative effect on predicting pixel-accurate masks.
RoIAlign：RoIPool [12]は、各RoIから小さな特徴マップ（7×7など）を抽出するための標準的な操作です。 RoIPoolは、最初に浮動小数点RoIを特徴マップの離散グラニュラリティに量子化し、次にこの量子化されたRoIは、それ自体が量子化される空間ビンに細分され、最後に各ビンがカバーする特徴値が集約されます（通常は最大プーリングによって） ）。量子化は、たとえば、[x / 16]を計算することにより、連続座標xで実行されます。ここで、16はフィーチャマップのストライドであり、[・]は丸めです。同様に、定量化は、ビンに分割するときに実行されます（たとえば、7×7）。これらの量子化により、RoIと抽出された特徴の間に不整合が生じます。これは、小さな翻訳に対してロバストな分類に影響を与えない可能性がありますが、ピクセル精度のマスクの予測に大きな悪影響を及ぼします。

To address this, we propose an RoIAlign layer that removes the harsh quantization of RoIPool, properly aligning the extracted features with the input.
これに対処するために、RoIPoolの過酷な量子化を取り除き、抽出された特徴を入力に適切に位置合わせするRoIAlignレイヤーを提案します。

Our proposed change is simple: we avoid any quantization of the RoI boundaries or bins (i.e., we use x/16 instead of [x/16]).
提案された変更は単純です。RoI境界またはビンの量子化を回避します（つまり、[x/16]の代わりにx/16を使用します）。

We use bilinear interpolation [22] to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and aggregate the result (using max or average), see Figure 3 for details.
バイリニア補間[22]を使用して、各RoIビンの4つの定期的にサンプリングされた場所で入力フィーチャの正確な値を計算し、結果を集計します（最大または平均を使用）。詳細については、図3を参照してください。

We note that the results are not sensitive to the exact sampling locations, or how many points are sampled, as long as no quantization is performed.
量子化が実行されない限り、結果は正確なサンプリング位置、またはサンプリングされるポイントの数に影響されないことに注意してください。

RoIAlign leads to large improvements as we show in §4.2. We also compare to the RoIWarp operation proposed in [10]. Unlike RoIAlign, RoIWarp overlooked the align- ment issue and was implemented in [10] as quantizing RoI just like RoIPool. So even though RoIWarp also adopts bilinear resampling motivated by [22], it performs on par with RoIPool as shown by experiments (more details in Ta- ble 2c), demonstrating the crucial role of alignment.
RoIAlignは、§4.2で示すように、大幅な改善につながります。また、[10]で提案されているRoIWarp操作と比較します。 RoIAlignとは異なり、RoIWarpはアライメントの問題を見落とし、RoIPoolと同じようにRoIを量子化するものとして[10]で実装されました。したがって、RoIWarpは[22]によって動機付けられた双一次リサンプリングも採用していますが、実験（表2cで詳細）に示されているように、RoIPoolと同等のパフォーマンスを示し、アライメントの重要な役割を示しています。

Network Architecture: To demonstrate the generality of our approach, we instantiate Mask R-CNN with multiple architectures. For clarity, we differentiate between: (i) the convolutional backbone architecture used for feature ex- traction over an entire image, and (ii) the network head for bounding-box recognition (classification and regression) and mask prediction that is applied separately to each RoI.
ネットワークアーキテクチャ：私たちのアプローチの一般性を示すために、複数のアーキテクチャでMaskR-CNNをインスタンス化します。明確にするために、（i）画像全体の特徴抽出に使用される畳み込みバックボーンアーキテクチャと、（ii）バウンディングボックス認識（分類と回帰）およびマスク予測用のネットワークヘッドを区別します。各RoI。

We denote the backbone architecture using the nomen- clature network-depth-features. We evaluate ResNet [19] and ResNeXt [45] networks of depth 50 or 101 layers. The original implementation of Faster R-CNN with ResNets [19] extracted features from the final convolutional layer of the 4-th stage, which we call C4. This backbone with ResNet-50, for example, is denoted by ResNet-50-C4. This is a common choice used in [19, 10, 21, 39].
ネットワーク深度機能という用語を使用してバックボーンアーキテクチャを示します。深さ50または101層のResNet[19]およびResNeXt[45]ネットワークを評価します。 ResNetsを使用したFasterR-CNNの元の実装[19]は、C4と呼ばれる第4ステージの最後の畳み込み層から特徴を抽出しました。たとえば、ResNet-50を備えたこのバックボーンは、ResNet-50-C4で表されます。これは[19、10、21、39]で使用される一般的な選択です。

We also explore another more effective backbone recently proposed by Lin et al. [27], called a Feature Pyramid Network (FPN).
また、Linらによって最近提案された別のより効果的なバックボーンについても説明します。 [27]、機能ピラミッドネットワーク（FPN）と呼ばれます。

FPN uses a top-down architecture with lateral connections to build an in-network feature pyramid from a single-scale input.
FPNは、横方向の接続を備えたトップダウンアーキテクチャを使用して、シングルスケール入力からネットワーク内機能ピラミッドを構築します。
Faster R-CNN with an FPN back- bone extracts RoI features from different levels of the feature pyramid according to their scale, but otherwise the rest of the approach is similar to vanilla ResNet.
FPNバックボーンを備えたより高速なR-CNNは、スケールに応じて機能ピラミッドのさまざまなレベルからRoI機能を抽出しますが、それ以外のアプローチはバニラResNetと同様です。
Using a ResNet-FPN backbone for feature extraction with Mask R- CNN gives excellent gains in both accuracy and speed. For further details on FPN, we refer readers to [27].
マスクR-CNNを使用した特徴抽出にResNet-FPNバックボーンを使用すると、精度と速度の両方で優れた向上が得られます。 FPNの詳細については、読者に[27]を参照してください。


For the network head we closely follow architectures presented in previous work to which we add a fully con- volutional mask prediction branch.
ネットワークヘッドについては、完全に畳み込みのマスク予測ブランチを追加する前の作業で提示されたアーキテクチャに厳密に従います。

Specifically, we ex- tend the Faster R-CNN box heads from the ResNet [19] and FPN [27] papers. Details are shown in Figure 4.
具体的には、ResNet[19]およびFPN[27]の論文からFasterR-CNNボックスヘッドを拡張します。詳細を図4に示します。

The head on the ResNet-C4 backbone includes the 5-th stage of ResNet (namely, the 9-layer ‘res5’ [19]), which is compute- intensive. For FPN, the backbone already includes res5 and thus allows for a more efficient head that uses fewer filters.
ResNet-C4バックボーンのヘッドには、計算集約型のResNetの第5ステージ（つまり、9層の「res5」[19]）が含まれています。

FPNの場合、バックボーンにはすでにres5が含まれているため、より少ないフィルターを使用するより効率的なヘッドが可能になります。

We note that our mask branches have a straightforward structure.
マスクブランチは単純な構造であることに注意してください。

More complex designs have the potential to im- prove performance but are not the focus of this work.
より複雑な設計はパフォーマンスを向上させる可能性がありますが、この作業の焦点では​​ありません。

14×14 14×14 ×256 ×80
mask
14×14 14×14 RoI ×256 ×4 ×256
28×28 28×28 ×256 ×80
mask
      4
7×7 7×7 ave RoI ×1024 res5 ×2048
Faster R-CNN w/ ResNet [19]
class 2048 box

Figure 4. Head Architecture: We extend two existing Faster R- CNN heads [19, 27]. Left/Right panels show the heads for the ResNet C4 and FPN backbones, from [19] and [27], respectively, to which a mask branch is added. Numbers denote spatial resolu- tion and channels. Arrows denote either conv, deconv, or fc layers as can be inferred from context (conv preserves spatial dimension while deconv increases it). All convs are 3×3, except the output conv which is 1×1, deconvs are 2×2 with stride 2, and we use ReLU [31] in hidden layers. Left: ‘res5’ denotes ResNet’s fifth stage, which for simplicity we altered so that the first conv oper- ates on a 7×7 RoI with stride 1 (instead of 14×14 / stride 2 as in [19]). Right: ‘×4’ denotes a stack of four consecutive convs.
図4.ヘッドアーキテクチャ：2つの既存のFaster R-CNNヘッドを拡張します[19、27]。左/右のパネルは、ResNet C4およびFPNバックボーンのヘッドをそれぞれ[19]および[27]から示しており、それにマスクブランチが追加されています。数字は空間分解能とチャネルを示します。矢印は、コンテキストから推測できるように、conv、deconv、またはfcレイヤーのいずれかを示します（convは空間次元を保持し、deconvはそれを増加させます）。 1×1である出力convを除いて、すべてのconvは3×3であり、deconvはストライド2で2×2であり、隠れ層でReLU[31]を使用します。左：「res5」はResNetの第5ステージを示します。簡単にするために、最初のコンバージョンがストライド1の7×7 RoIで動作するように変更しました（[19]のように14×14 /ストライド2ではありません）。右：「×4」は、4つの連続するコンバージョンのスタックを示します。

### 3.1. Implementation Details

We set hyper-parameters following existing Fast/Faster R-CNN work [12, 36, 27]. Although these decisions were made for object detection in original papers [12, 36, 27], we found our instance segmentation system is robust to them.
既存のFast/Faster R-CNN作業[12、36、27]に従ってハイパーパラメータを設定します。これらの決定は、元の論文[12、36、27]でのオブジェクト検出のために行われましたが、インスタンスセグメンテーションシステムはそれらに対して堅牢であることがわかりました。

Training: AsinFastR-CNN,anRoIisconsideredpositive if it has IoU with a ground-truth box of at least 0.5 and negative otherwise. The mask loss Lmask is defined only on positive RoIs. The mask target is the intersection between an RoI and its associated ground-truth mask.
トレーニング：AsinFastR-CNN、anRoIは、グラウンドトゥルースボックスが0.5以上のIoUがある場合は正、それ以外の場合は負と見なされます。マスク損失Lmaskは、正のRoIでのみ定義されます。マスクターゲットは、RoIとそれに関連するグラウンドトゥルースマスクの共通部分です。

We adopt image-centric training [12]. Images are resized such that their scale (shorter edge) is 800 pixels [27]. Each mini-batch has 2 images per GPU and each image has N sampled RoIs, with a ratio of 1:3 of positive to negatives [12]. N is 64 for the C4 backbone (as in [12, 36]) and 512 for FPN (as in [27]). We train on 8 GPUs (so effective mini- batch size is 16) for 160k iterations, with a learning rate of 0.02 which is decreased by 10 at the 120k iteration. We use a weight decay of 0.0001 and momentum of 0.9. With ResNeXt [45], we train with 1 image per GPU and the same number of iterations, with a starting learning rate of 0.01.

The RPN anchors span 5 scales and 3 aspect ratios, fol- lowing [27]. For convenient ablation, RPN is trained sep- arately and does not share features with Mask R-CNN, un- less specified. For every entry in this paper, RPN and Mask R-CNN have the same backbones and so they are shareable.

Inference: At test time, the proposal number is 300 for the C4 backbone (as in [36]) and 1000 for FPN (as in [27]). We run the box prediction branch on these proposals, followed by non-maximum suppression [14]. The mask branch is then applied to the highest scoring 100 detection boxes. Al- though this differs from the parallel computation used in training, it speeds up inference and improves accuracy (due to the use of fewer, more accurate RoIs). The mask branch can predict K masks per RoI, but we only use the k-th mask, where k is the predicted class by the classification branch. The m×m floating-number mask output is then resized to the RoI size, and binarized at a threshold of 0.5.
Note that since we only compute masks on the top 100 detection boxes, Mask R-CNN adds a small overhead to its Faster R-CNN counterpart (e.g., ∼20% on typical models).


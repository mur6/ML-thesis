# MultiMAE: Multi-modal Multi-task Masked Autoencoders


## 3. Method Description

In this Section, we describe the Multi-modal Multi-task Masked Autoencoder (MultiMAE) architecture (illustrated in Fig. 2), as well as the pre-training strategy in more detail. We first give an architectural overview of both the multi- modal encoder (Sec. 3.1) and multi-task decoders (Sec. 3.2). We then describe our multi-modal token sampling strategy (Sec. 3.3) and introduce the pseudo labeled tasks we use for pre-training (Sec. 3.4). Finally, we display the most impor- tant pre-training details (Sec. 3.5).

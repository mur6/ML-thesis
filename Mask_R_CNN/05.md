5. Mask R-CNN for Human Pose Estimation

Our framework can easily be extended to human pose estimation. We model a keypoint’s location as a one-hot mask, and adopt Mask R-CNN to predict K masks, one for each of K keypoint types (e.g., left shoulder, right elbow). This task helps demonstrate the flexibility of Mask R-CNN.
私たちのフレームワークは、人間のポーズ推定に簡単に拡張できます。キーポイントの位置をワンホットマスクとしてモデル化し、マスクR-CNNを採用して、K個のキーポイントタイプ（左肩、右肘など）ごとに1つずつ、K個のマスクを予測します。このタスクは、マスクR-CNNの柔軟性を示すのに役立ちます。

We note that minimal domain knowledge for human pose is exploited by our system, as the experiments are mainly to demonstrate the generality of the Mask R-CNN framework. We expect that domain knowledge (e.g., modeling struc- tures [6]) will be complementary to our simple approach.
実験は主にマスクR-CNNフレームワークの一般性を実証するためのものであるため、人間のポーズに関する最小限のドメイン知識がシステムによって活用されていることに注意してください。ドメイン知識（たとえば、構造のモデリング[6]）は、私たちの単純なアプローチを補完するものになると期待しています。

Implementation Details: We make minor modifications to the segmentation system when adapting it for keypoints. For each of the K keypoints of an instance, the training target is a one-hot m × m binary mask where only a single pixel is labeled as foreground. During training, for each vis- ible ground-truth keypoint, we minimize the cross-entropy loss over an m2-way softmax output (which encourages a single point to be detected). We note that as in instance seg- mentation, the K keypoints are still treated independently.
実装の詳細：キーポイントに適合させる際に、セグメンテーションシステムに小さな変更を加えます。インスタンスのK個のキーポイントごとに、トレーニングターゲットはワンホットm×mバイナリマスクであり、1つのピクセルのみが前景としてラベル付けされます。トレーニング中、目に見えるグラウンドトゥルースキーポイントごとに、m2ウェイソフトマックス出力（単一ポイントの検出を促進します）でのクロスエントロピー損失を最小限に抑えます。インスタンスのセグメント化と同様に、Kキーポイントは引き続き独立して扱われることに注意してください。

We adopt the ResNet-FPN variant, and the keypoint head architecture is similar to that in Figure 4 (right). The key- point head consists of a stack of eight 3×3 512-d conv lay- ers, followed by a deconv layer and 2× bilinear upscaling, producing an output resolution of 56×56. We found that a relatively high resolution output (compared to masks) is required for keypoint-level localization accuracy.
ResNet-FPNバリアントを採用しており、キーポイントヘッドのアーキテクチャは図4（右）のアーキテクチャと似ています。キーポイントヘッドは、8つの3×3 512-dコンバージョンレイヤーのスタックと、それに続くデコンバーレイヤーと2×バイリニアアップスケーリングで構成され、56×56の出力解像度を生成します。キーポイントレベルのローカリゼーションの精度には、（マスクと比較して）比較的高い解像度の出力が必要であることがわかりました。

Models are trained on all COCO trainval35k im- ages that contain annotated keypoints. To reduce overfit- ting, as this training set is smaller, we train using image scales randomly sampled from [640, 800] pixels; inference is on a single scale of 800 pixels. We train for 90k iterations, starting from a learning rate of 0.02 and reducing it by 10 at 60k and 80k iterations. We use bounding-box NMS with a threshold of 0.5. Other details are identical as in §3.1.
モデルは、注釈付きのキーポイントを含むすべてのCOCOtrainval35kイメージでトレーニングされます。過剰適合を減らすために、このトレーニングセットは小さいため、[640、800]ピクセルからランダムにサンプリングされた画像スケールを使用してトレーニングします。推論は800ピクセルの単一スケールです。 0.02の学習率から開始し、60kおよび80kの反復で10ずつ減らす、90kの反復でトレーニングします。しきい値が0.5のバウンディングボックスNMSを使用します。その他の詳細は§3.1と同じです。

Main Results and Ablations: We evaluate the person key- point AP (APkp) and experiment with a ResNet-50-FPN backbone; more backbones will be studied in the appendix. Table 4 shows that our result (62.7 APkp) is 0.9 points higher than the COCO 2016 keypoint detection winner [6] that uses a multi-stage processing pipeline (see caption of Ta- ble 4). Our method is considerably simpler and faster.
主な結果とアブレーション：個人キーポイントAP（APkp）を評価し、ResNet-50-FPNバックボーンで実験します。より多くのバックボーンが付録で研究されます。表4は、結果（62.7 APkp）が、多段階処理パイプラインを使用するCOCO 2016キーポイント検出の勝者[6]よりも0.9ポイント高いことを示しています（表4のキャプションを参照）。私たちの方法はかなり簡単で高速です。

More importantly, we have a unified model that can si-multaneously predict boxes, segments, and keypoints while running at 5 fps. Adding a segment branch (for the per- son category) improves the APkp to 63.1 (Table 4) on test-dev. More ablations of multi-task learning on minival are in Table 5. Adding the mask branch to the box-only (i.e., Faster R-CNN) or keypoint-only versions consistently improves these tasks. However, adding the keypoint branch reduces the box/mask AP slightly, suggest- ing that while keypoint detection benefits from multitask training, it does not in turn help the other tasks.
さらに重要なのは、5 fpsで実行しながら、ボックス、セグメント、キーポイントを同時に予測できる統合モデルがあることです。セグメントブランチ（個人カテゴリ用）を追加すると、test-devでAPkpが63.1（表4）に向上します。ミニバルでのマルチタスク学習のより多くのアブレーションを表5に示します。ボックスのみ（つまり、より高速なR-CNN）またはキーポイントのみのバージョンにマスクブランチを追加すると、これらのタスクが一貫して改善されます。ただし、キーポイントブランチを追加すると、ボックス/マスクAPがわずかに減少します。これは、キーポイント検出はマルチタスクトレーニングの恩恵を受けますが、他のタスクには役立たないことを示しています。

Nevertheless, learning all three tasks jointly enables a unified system to efficiently predict all outputs simultaneously (Figure 7).
それでも、3つのタスクすべてを一緒に学習することで、統合システムはすべての出力を同時に効率的に予測できます（図7）。

We also investigate the effect of RoIAlign on keypoint detection (Table 6). Though this ResNet-50-FPN backbone has finer strides (e.g., 4 pixels on the finest level), RoIAlign still shows significant improvement over RoIPool and in- creases APkp by 4.4 points. This is because keypoint detec- tions are more sensitive to localization accuracy. This again indicates that alignment is essential for pixel-level localiza- tion, including masks and keypoints.
また、キーポイント検出に対するRoIAlignの影響を調査します（表6）。このResNet-50-FPNバックボーンは、より細かいストライド（たとえば、最高レベルで4ピクセル）を備えていますが、RoIAlignは、RoIPoolに比べて大幅な改善を示し、APkpを4.4ポイント増加させます。これは、キーポイントの検出がローカリゼーションの精度に敏感であるためです。これもまた、マスクやキーポイントを含むピクセルレベルのローカリゼーションに位置合わせが不可欠であることを示しています。

Given the effectiveness of Mask R-CNN for extracting object bounding boxes, masks, and keypoints, we expect it be an effective framework for other instance-level tasks.
オブジェクトの境界ボックス、マスク、およびキーポイントを抽出するためのMask R-CNNの有効性を考えると、他のインスタンスレベルのタスクの効果的なフレームワークであることが期待されます。

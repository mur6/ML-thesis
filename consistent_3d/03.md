3 METHODOLOGY

3.1 Overview

Our method enables end-to-end learning of accurate and consistent 3D hand reconstruction from video sequences in a self-supervised manner through S2HAND(V) (Section 3.3), which is based on S2HAND (Section 3.2). The overview is illustrated in Fig. 2.
私たちの方法は、S2HAND（セクション3.2）に基づくS2HAND（V）（セクション3.3）を介して、ビデオシーケンスからの正確で一貫性のある3D手の再構成のエンドツーエンドの学習を可能にします。概要を図2に示します。

The S2HAND model takes an image as input and gener- ates a textured 3D hand represented by pose, shape and tex- ture, along with corresponding lighting, camera viewpoint (Section 3.2.1 and 3.2.2) and multiple 2D representations in the image space (Section 3.2.3). Some valid loss functions and regularization terms (Section 3.2.4) are explored to train the network without using ground truth annotations. The S2HAND(V) model takes video sequences as input and pro- duces consistent sequential outputs from multiple S2HAND models where their weights are shared. A quaternion loss (Section 3.3.1) and a T&S consistency loss (Section 3.3.2) are designed to train the network with temporal constraints. We describe the proposed method in detail as below.
S2HANDモデルは、画像を入力として受け取り、ポーズ、形状、テクスチャで表されるテクスチャ3Dハンドを、対応する照明、カメラの視点（セクション3.2.1および3.2.2）、および画像内の複数の2D表現とともに生成します。スペース（セクション3.2.3）。グラウンドトゥルースアノテーションを使用せずにネットワークをトレーニングするために、いくつかの有効な損失関数と正則化項（セクション3.2.4）が検討されています。 S2HAND（V）モデルは、ビデオシーケンスを入力として受け取り、重みが共有される複数のS2HANDモデルから一貫したシーケンシャル出力を生成します。クォータニオン損失（セクション3.3.1）とT＆S整合性損失（セクション3.3.2）は、時間的制約を使用してネットワークをトレーニングするように設計されています。提案する方法を以下に詳細に説明する。

3.2 Self-supervised Hand Reconstruction from image collections
The S2HAND model learns self-supervised 3D hand recon- struction from image collections via training a 3D hand reconstruction network with the help of a 2D keypoints estimator.

3.2.1 Deep Hand Encoding
Given a image I contains a hand, the 3D hand reconstruc- tion network first uses an EfficientNet-b0 backbone [65] to encode the image into a geometry semantic code vector x and a texture semantic code vector y. The geometry seman- tic code vector x parameterizes the hand pose θ ∈ R30, shape β ∈ R10, scale s ∈ R1, rotation R ∈ R3 and translation T ∈ R3 in a unified manner: x = (θ,β,s,R,T). The texture semantic code vector y parameterizes the hand texture C ∈ R778×3 and scene lighting L ∈ R11 in a unified manner: y = (C, L).


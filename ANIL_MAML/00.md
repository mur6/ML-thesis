An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning.
機械学習の重要な研究の方向性は、数ショットの学習に取り組むためのメタ学習アルゴリズムの開発に集中しています。

An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks.
特に成功したアルゴリズムは、モデルにとらわれないメタ学習（MAML）です。これは、2つの最適化ループで構成される方法で、外側のループがメタ初期化を見つけ、そこから内側のループが新しいタスクを効率的に学習できます。

Despite MAML’s popularity, a fundamental open question remains – is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta-initialization already containing high quality features?
MAMLの人気にもかかわらず、基本的な未解決の質問が残っています。メタ初期化が迅速な学習（表現の大規模で効率的な変更）のために準備されているため、またはメタ初期化にすでに高品質が含まれている機能の再利用によるMAMLの有効性です。

We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor.
特徴？アブレーション研究と潜在表現の分析を介してこの質問を調査し、機能の再利用が支配的な要因であることを発見します。

This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network.
これにより、ANIL（Almost No Inner Loop）アルゴリズムが実現します。これは、基礎となるニューラルネットワークの（タスク固有の）ヘッドを除くすべての内部ループを削除するMAMLの簡略化です。

ANIL matches MAML’s performance on benchmark few-shot image classification and RL and offers computational improvements over MAML.
ANILは、ベンチマークの数ショット画像分類とRLでのMAMLのパフォーマンスと一致し、MAMLよりも計算が改善されています。

We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm).
ネットワークのヘッドとボディの正確な寄与をさらに調査し、テストタスクのパフォーマンスは、学習した機能の品質によって完全に決定され、ネットワークのヘッドも削除できることを示しています（NILアルゴリズム）。

We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.
最後に、メタ学習アルゴリズムの高速学習と機能の再利用に関する質問について、より広範に説明します。

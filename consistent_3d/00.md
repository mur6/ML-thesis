Abstract

We present a method for reconstructing accurate and consistent 3D hands from a monocular video. We observe that detected 2D hand keypoints and the image texture provide important cues about the geometry and texture of the 3D hand, which can reduce or even eliminate the requirement on 3D hand annotation. Thus we propose S2HAND, a self-supervised 3D hand reconstruction model, that can jointly estimate pose, shape, texture, and the camera viewpoint from a single RGB input through the supervision of easily accessible 2D detected keypoints. We leverage the continuous hand motion information contained in the unlabeled video data and propose S2HAND(V), which uses a set of weights shared S2HAND to process each frame and exploits additional motion, texture, and shape consistency constrains to promote more accurate hand poses and more consistent shapes and textures. Experiments on benchmark datasets demonstrate that our self-supervised approach produces comparable hand reconstruction performance compared with the recent full-supervised methods in single-frame as input setup, and notably improves the reconstruction accuracy and consistency when using video training data.

単眼ビデオから正確で一貫性のある3D手を再構築する方法を紹介します。検出された2D手のキーポイントと画像テクスチャが、3D手の形状とテクスチャに関する重要な手がかりを提供し、3D手の注釈の要件を軽減または排除できることを確認しました。そこで、簡単にアクセスできる2D検出キーポイントの監視を通じて、単一のRGB入力からポーズ、形状、テクスチャ、およびカメラの視点を共同で推定できる、自己監視3D手の再構成モデ​​ルであるS2HANDを提案します。ラベルのないビデオデータに含まれる連続的な手の動きの情報を活用し、S2HAND（V）を提案します。これは、共有されたS2HANDの重みのセットを使用して各フレームを処理し、追加の動き、テクスチャ、および形状の一貫性の制約を利用して、より正確な手のポーズを促進します。より一貫した形状とテクスチャ。ベンチマークデータセットでの実験は、私たちの自己監視アプローチが、入力セットアップとしての単一フレームでの最近の完全監視方法と比較して同等の手の再構成パフォーマンスを生成し、ビデオトレーニングデータを使用する場合の再構成の精度と一貫性を特に向上させることを示しています。

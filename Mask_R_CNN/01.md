# Mask R-CNN
## 1. Introduction
The vision community has rapidly improved object de- tection and semantic segmentation results over a short pe- riod of time.
ビジョンコミュニティは、短期間でオブジェクトの検出とセマンティックセグメンテーションの結果を急速に改善しました。
In large part, these advances have been driven by powerful baseline systems, such as the Fast/Faster R- CNN [12, 36] and Fully Convolutional Network (FCN) [30] frameworks for object detection and semantic segmentation, respectively.
これらの進歩の大部分は、オブジェクト検出とセマンティックセグメンテーションのためのFast / Faster R- CNN [12、36]およびFully Convolutional Network（FCN）[30]フレームワークなどの強力なベースラインシステムによって推進されてきました。
These methods are conceptually intuitive and offer flexibility and robustness, together with fast train- ing and inference time.
これらの方法は概念的に直感的であり、高速なトレーニングと推論時間とともに、柔軟性と堅牢性を提供します。
Our goal in this work is to develop a comparably enabling framework for instance segmentation.
この作業での私たちの目標は、インスタンスのセグメンテーションのための比較的有効なフレームワークを開発することです。

Instance segmentation is challenging because it requires the correct detection of all objects in an image while also precisely segmenting each instance.
インスタンスのセグメンテーションは、各インスタンスを正確にセグメント化すると同時に、画像内のすべてのオブジェクトを正しく検出する必要があるため、困難です。

It therefore combines elements from the classical computer vision tasks of object detection, where the goal is to classify individual objects and localize each using a bounding box, and semantic segmentation, where the goal is to classify each pixel into a fixed set of categories without differentiating object instances.
したがって、個々のオブジェクトを分類し、境界ボックスを使用してそれぞれをローカライズすることを目的とするオブジェクト検出の従来のコンピュータビジョンタスクの要素と、各ピクセルを固定セットに分類することを目的とするセマンティックセグメンテーションの要素を組み合わせます。オブジェクトインスタンスを区別せずにカテゴリを分類します。
Given this, one might expect a complex method is required to achieve good results.
これを考えると、良好な結果を得るには複雑な方法が必要になると予想される場合があります。
However, we show that a surprisingly simple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results.
ただし、驚くほどシンプルで柔軟性があり、高速なシステムが、以前の最先端のインスタンスセグメンテーションの結果を超えることができることを示します。

Our method, called Mask R-CNN, extends Faster R-CNN [36] by adding a branch for predicting segmentation masks on each Region of Interest (RoI), in parallel with the existing branch for classification and bounding box regression (Figure 1).
Mask R-CNNと呼ばれるこの方法は、分類とバウンディングボックス回帰のための既存のブランチと並行して、各関心領域（RoI）でセグメンテーションマスクを予測するためのブランチを追加することにより、FasterR-CNN[36]を拡張します。（図1）。
The mask branch is a small FCN applied to each RoI, predicting a segmentation mask in a pixel-to- pixel manner. Mask R-CNN is simple to implement and train given the Faster R-CNN framework, which facilitates a wide range of flexible architecture designs.
マスクブランチは、各RoIに適用される小さなFCNであり、ピクセルごとにセグメンテーションマスクを予測します。マスクR-CNNは、Faster R-CNNフレームワークを使用すると、実装とトレーニングが簡単です。これにより、さまざまな柔軟なアーキテクチャ設計が容易になります。
Additionally, the mask branch only adds a small computational overhead, enabling a fast system and rapid experimentation.
さらに、マスクブランチはわずかな計算オーバーヘッドを追加するだけであり、高速システムと迅速な実験を可能にします。

In principle Mask R-CNN is an intuitive extension of Faster R-CNN, yet constructing the mask branch properly is critical for good results.
原則として、MaskR-CNNはFasterR-CNNの直感的な拡張ですが、良好な結果を得るには、マスクブランチを適切に構築することが重要です。
Most importantly, Faster R-CNN was not designed for pixel-to-pixel alignment between network inputs and outputs.
最も重要なことは、Faster R-CNNは、ネットワーク入力と出力の間のピクセル間の位置合わせ用に設計されていないことです。
This is most evident in how RoIPool [18, 12], the de facto core operation for attending to instances, performs coarse spatial quantization for feature extraction.
これは、インスタンスに注意を向けるための事実上のコア操作であるRoIPool [18、12]が、特徴抽出のために粗い空間量子化を実行する方法で最も明白です。
To fix the misalignment, we pro- pose a simple, quantization-free layer, called RoIAlign, that faithfully preserves exact spatial locations.
ミスアライメントを修正するために、RoIAlignと呼ばれる、正確な空間位置を忠実に保持する、量子化のない単純なレイヤーを提案します。

Despite being a seemingly minor change, RoIAlign has a large impact: it improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics. Second, we found it essential to decouple mask and class prediction: we predict a binary mask for each class independently, without competition among classes, and rely on the network’s RoI classification branch to predict the category. In contrast, FCNs usually perform per-pixel multi-class categorization, which couples segmentation and classification, and based on our experiments works poorly for instance segmentation.
一見小さな変更であるにもかかわらず、RoIAlignには大きな影響があります。これにより、マスクの精度が比較的10％から50％向上し、より厳密なローカリゼーションメトリックの下でより大きな向上が見られます。次に、マスクとクラス予測を分離することが不可欠であることがわかりました。クラス間で競合することなく、各クラスのバイナリマスクを個別に予測し、ネットワークのRoI分類ブランチに依存してカテゴリを予測します。対照的に、FCNは通常、ピクセルごとのマルチクラス分類を実行します。これは、セグメンテーションと分類を結合し、私たちの実験に基づくと、たとえばセグメンテーションではうまく機能しません。

Without bells and whistles, Mask R-CNN surpasses all previous state-of-the-art single-model results on the COCO instance segmentation task [28], including the heavily- engineered entries from the 2016 competition winner. As a by-product, our method also excels on the COCO object detection task. In ablation experiments, we evaluate multi- ple basic instantiations, which allows us to demonstrate its robustness and analyze the effects of core factors.
ベルとホイッスルがなければ、Mask R-CNNは、2016年のコンペティション優勝者からの高度に設計されたエントリを含む、COCOインスタンスセグメンテーションタスク[28]での以前のすべての最先端のシングルモデルの結果を上回ります。副産物として、私たちの方法はCOCOオブジェクト検出タスクにも優れています。アブレーション実験では、複数の基本的なインスタンス化を評価します。これにより、その堅牢性を実証し、コアファクターの影響を分析できます。

Our models can run at about 200ms per frame on a GPU, and training on COCO takes one to two days on a single 8-GPU machine. We believe the fast train and test speeds, together with the framework’s flexibility and accuracy, will benefit and ease future research on instance segmentation.
私たちのモデルは、GPUでフレームあたり約200msで実行でき、COCOでのトレーニングは単一の8-GPUマシンで1〜2日かかります。フレームワークの柔軟性と正確性とともに、高速トレインとテスト速度は、インスタンスのセグメンテーションに関する将来の研究に役立ち、容易になると信じています。

Finally, we showcase the generality of our framework via the task of human pose estimation on the COCO key- point dataset [28]. By viewing each keypoint as a one-hot binary mask, with minimal modification Mask R-CNN can be applied to detect instance-specific poses. Mask R-CNN surpasses the winner of the 2016 COCO keypoint compe- tition, and at the same time runs at 5 fps. Mask R-CNN, therefore, can be seen more broadly as a flexible framework for instance-level recognition and can be readily extended to more complex tasks.
最後に、COCOキーポイントデータセットでの人間の姿勢推定のタスクを介して、フレームワークの一般性を示します[28]。各キーポイントをワンホットバイナリマスクとして表示することにより、最小限の変更でマスクR-CNNを適用して、インスタンス固有のポーズを検出できます。マスクR-CNNは、2016年のCOCOキーポイントコンペティションの勝者を上回り、同時に5fpsで実行されます。したがって、マスクR-CNNは、インスタンスレベルの認識のための柔軟なフレームワークとしてより広く見ることができ、より複雑なタスクに容易に拡張できます。

We have released code to facilitate future research.
将来の研究を容易にするためのコードをリリースしました。

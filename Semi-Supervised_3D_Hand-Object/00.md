Estimating 3D hand and object pose from a single image is an extremely challenging problem: hands and objects are often self-occluded during interactions, and the 3D annotations are scarce as even humans cannot directly label the ground-truths from a single image perfectly.
単一の画像から3Dの手とオブジェクトのポーズを推定することは、非常に難しい問題です。手とオブジェクトは、インタラクション中に自己閉塞することが多く、人間でさえ単一のグラウンドトゥルースに直接ラベルを付けることができないため、3D注釈はほとんどありません。完璧にイメージ。

To tackle these challenges, we propose a unified framework for estimating the 3D hand and object poses with semi- supervised learning.

これらの課題に取り組むために、半教師あり学習で3Dの手とオブジェクトのポーズを推定するための統一されたフレームワークを提案します。

We build a joint learning framework where we perform explicit contextual reasoning between hand and object representations by a Transformer.
Transformerによる手とオブジェクトの表現の間で明示的なコンテキスト推論を実行する共同学習フレームワークを構築します。

Going beyond limited 3D annotations in a single image, we lever- age the spatial-temporal consistency in large-scale hand- object videos as a constraint for generating pseudo labels in semi-supervised learning.
単一画像内の限られた3D注釈を超えて、半教師あり学習で疑似ラベルを生成するための制約として、大規模なハンドオブジェクトビデオの時空間一貫性を活用します。

Our method not only improves hand pose estimation in challenging real-world dataset, but also substantially improve the object pose which has fewer ground-truths per instance. By training with large-scale diverse videos, our model also generalizes better across multiple out-of-domain datasets. Project page and code: https://stevenlsw.github.io/Semi-Hand-Object.

私たちの方法は、挑戦的な現実世界のデータセットでの手のポーズ推定を改善するだけでなく、インスタンスあたりのグラウンドトゥルースが少ないオブジェクトポーズも大幅に改善します。大規模で多様なビデオを使用してトレーニングすることにより、モデルは複数のドメイン外データセット間でより一般化されます。プロジェクトページとコード：

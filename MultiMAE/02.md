# MultiMAE: Multi-modal Multi-task Masked Autoencoders

## 2. Related Work
Masked image prediction consists of learning useful rep- resentations by learning to reconstruct images corrupted by masking. This approach was pioneered with denoising au- toencoders [86] and context encoders [64]. With the intro- duction of Vision Transformers (ViT) [26] and motivated by the success of BERT [24] in NLP, many recent works propose a variety of masked image prediction methods for pre-training vision models in a self-supervised way, using reconstruction targets such as pixels [6, 16, 26, 29, 35, 93], discrete tokens [9, 103], and (deep) features [8, 88]. These methods scale very well and achieve strong results on var- ious downstream tasks including motor control [91]. In particular, the masked autoencoder (MAE) [35] approach accelerates pre-training by using an asymmetric architec- ture consisting of a large encoder that operates only on un- masked patches followed by a lightweight decoder that re- constructs the masked patches from the latent representation and mask tokens. Our approach leverages the efficiency of the MAE approach and extends it to multi-modal and multi- task settings.

Multi-modal learning involves building models capable of relating information from multiple sources. It can either involve training separate encoders or one unified architec- ture (e.g., a Transformer [85]) to operate on modalities such as images and text [4, 14, 18, 39, 42â€“45, 56, 57, 76, 79, 94], video and audio [5,41,60,62], video, text and audio [3], and depth, images and video [32]. Our work proposes a simple approach to pre-train Transformers on multiple dense visual modalities and produce strong cross-modal interaction. Un- like most prior work which assumes that all modalities are available during inference, our approach is designed to per- form well on any subset of the pre-training modalities.
Related to MultiMAE are several works that perform multi-modal autoencoding [61,72,77,78,89]. Our approach differs from them in that we use a more flexible architecture and perform masked autoencoding to learn cross-modal pre- dictive coding among optional inputs (as demonstrated in Fig. 1).

Multi-task learning consists of training models to predict multiple output domains from a single input [13, 28, 46]. In computer vision, the input is usually an RGB image. A common approach for multi-task learning is to use a single encoder to learn a shared representation followed by mul- tiple task-specific decoders [31, 84]. These methods differ from our approach as we use multiple tasks in both the input and the output along with masking.
In addition, many works study the importance of task di- versity to improve transfer performance [31, 58, 70, 82, 99]. These works argue that learning from one task alone is in- sufficient and that a set of tasks can more effectively cover the many possible downstream tasks in vision. Our pre-training method operates on multiple tasks to learn more general representations capable of covering multiple down- stream tasks.


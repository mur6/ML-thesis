4. Hand-Object Joint Learning Framework

Our hand-object joint learning framework is presented in Figure 2. We use FPN [41] with ResNet-50 [26] as the back- bone and extract hand and object features Fh and Fo into RH×W×C withtheRoIAlign[25],giventheircorresponding bounding boxes. Then we apply the contextual reasoning be- tween the two features and send the enhanced feature maps with strengthened interactive context information into the hand and the object decoders respectively, which output the 3D hand mesh and 6-Dof object pose. The total loss func- tion of the system is the sum from two decoder branches L = Lhand + Lobject. The contextual reasoning module, hand and object decoders, and training losses Lhand, Lobject will be discussed in the following sections.

4.1. Contextual Reasoning

We adopt the Transformer architecture to exploit the syn- ergy between hand and object features via the contextual reasoning (CR) module as shown in Figure 3, where the query positions in object features could be enhanced by fus- ing information from the interaction region.
Inside the module, we take object features Fo as query and hand-object intersecting regions F inter as key and value to model their pairwise relations on the top of RoIAlign [25]. We first apply three separate 1-D convolutions parameterized by Wq, Wk, and Wv on the input features to get the query, key, and value embedding Q, K, and V :

数式(1)


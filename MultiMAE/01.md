# MultiMAE: Multi-modal Multi-task Masked Autoencoders

## 1. Introduction
Masked Autoencoders (MAEs) [35] have recently been
demonstrated to be a powerful, yet conceptually simple
and efficient, self-supervised pre-training strategy for Vision Transformers [26] (ViTs). Their training objective is
to mask-out a high number of patches in an input image
and to predict the missing regions. To that end, only the
small number of non-masked patches are first processed using a Transformer encoder [85], and then decoded with a
light-weight Transformer that reconstructs the original image. To solve this task sufficiently well, it is assumed [35]

1 INTRODUCTION

A central problem in machine learning is few-shot learning, where new tasks must be learned with a very limited number of labelled datapoints. A significant body of work has looked at tackling this challenge using meta-learning approaches (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Santoro et al., 2016; Ravi and Larochelle, 2016; Nichol and Schulman, 2018).
機械学習の中心的な問題は、数ショットの学習です。この場合、ラベル付けされたデータポイントの数が非常に限られているため、新しいタスクを学習する必要があります。メタ学習アプローチを使用してこの課題に取り組むことを検討した多くの研究があります（Koch et al。、2015; Vinyals et al。、2016; Snell et al。、2017; Finn et al。、2017; Santoro et al。、 2016; Ravi and Larochelle、2016; Nichol and Schulman、2018）。

Broadly speaking, these approaches define a family of tasks, some of which are used for training and others solely for evaluation. A proposed meta-learning algorithm then looks at learning properties that generalize across the different training tasks, and result in fast and efficient learning of the evaluation tasks.
大まかに言えば、これらのアプローチは一連のタスクを定義し、その一部はトレーニングに使用され、その他は評価のみに使用されます。次に、提案されたメタ学習アルゴリズムは、さまざまなトレーニングタスク全体で一般化する学習プロパティを調べ、評価タスクの高速で効率的な学習を実現します。

One highly successful meta-learning algorithm has been Model Agnostic Meta-Learning (MAML) (Finn et al., 2017).
非常に成功したメタ学習アルゴリズムの1つは、モデルにとらわれないメタ学習（MAML）です（Finn et al。、2017）。

At a high level, the MAML algorithm is comprised of two optimization loops.
大まかに言えば、MAMLアルゴリズムは2つの最適化ループで構成されています。

The outer loop (in the spirit of meta-learning) aims to find an effective meta-initialization, from which the inner loop can perform efficient adaptation – optimize parameters to solve new tasks with very few labelled examples.
外側のループ（メタ学習の精神）は、効果的なメタ初期化を見つけることを目的としています。このメタ初期化から、内側のループは効率的な適応を実行できます。パラメーターを最適化して、ラベル付けされた例がほとんどない新しいタスクを解決します。

This algorithm, with deep neural networks as the underlying model, has been highly influential, with significant follow on work, such as first order variants (Nichol and Schulman, 2018), probabilistic extensions (Finn et al., 2018), augmentation with generative modelling (Rusu et al., 2018), and many others (Hsu et al., 2018; Finn and Levine, 2017; Grant et al., 2018; Triantafillou et al., 2019).
基礎となるモデルとしてディープニューラルネットワークを使用するこのアルゴリズムは、非常に影響力があり、一次バリアント（Nichol and Schulman、2018）、確率的拡張（Finn et al。、2018）、生成による拡張などの重要なフォローアップがあります。モデリング（Rusu et al。、2018）、および他の多く（Hsu et al。、2018; Finn and Levine、2017; Grant et al。、2018; Triantafillou et al。、2019）。

Despite the popularity of MAML, and the numerous followups and extensions, there remains a fundamental open question on the basic algorithm.
MAMLの人気、および多数のフォローアップと拡張にもかかわらず、基本的なアルゴリズムに関する基本的な未解決の質問が残っています。

Does the meta-initialization learned by the outer loop result in rapid learning on unseen test tasks (efficient but significant changes in the representations) or is the success primarily due to feature reuse (with the meta-initialization already providing high quality representations)?
外側のループによって学習されたメタ初期化は、目に見えないテストタスクの迅速な学習をもたらしますか（表現の効率的ですが重要な変更）、または主に機能の再利用による成功ですか（メタ初期化はすでに高品質の表現を提供しています）？

In this paper, we explore this question and its many surprising consequences.
このホワイトペーパーでは、この質問とその多くの驚くべき結果について説明します。

Our main contributions are:
私たちの主な貢献は次のとおりです。

• We perform layer freezing experiments and latent representational analysis of MAML, finding that feature reuse is the predominant reason for efficient learning.
•層凍結実験とMAMLの潜在的表現分析を実行し、機能の再利用が効率的な学習の主な理由であることを発見しました。

• Based on these results, we propose the ANIL (Almost No Inner Loop) algorithm, a significant simplification to MAML that removes the inner loop updates for all but the head (final layer) of a neural network during training and inference.
•これらの結果に基づいて、ANIL（Almost No Inner Loop）アルゴリズムを提案します。これは、トレーニングおよび推論中にニューラルネットワークのヘッド（最終層）を除くすべての内部ループの更新を削除するMAMLの大幅な簡略化です。

ANIL performs identically to MAML on standard benchmark few-shot classification and RL tasks and offers computational benefits over MAML.
ANILは、標準のベンチマーク数ショット分類およびRLタスクでMAMLと同じように実行され、MAMLよりも計算上の利点があります。

• We study the effect of the head of the network, finding that once training is complete, the head can be removed, and the representations can be used without adaptation to perform unseen tasks, which we call the No Inner Loop (NIL) algorithm.
•ネットワークのヘッドの効果を調査し、トレーニングが完了するとヘッドを削除でき、表現を適応せずに使用して、見えないタスクを実行できることを発見しました。これをNo Inner Loop（NIL）アルゴリズムと呼びます。

• We study different training regimes, e.g. multiclass classification, multitask learning, etc, and find that the task specificity of MAML/ANIL at training facilitate the learning of better features. We also find that multitask training, a popular baseline with no task specificity, performs worse than random features.
•さまざまなトレーニング体制を研究します。マルチクラス分類、マルチタスク学習など、トレーニング時のMAML/ANILのタスクの特異性がより優れた機能の学習を促進することを発見します。また、タスクの特異性がない人気のあるベースラインであるマルチタスクトレーニングは、ランダムな機能よりもパフォーマンスが悪いことがわかります。

• We discuss rapid learning and feature reuse in the context of other meta-learning approaches.
•他のメタ学習アプローチのコンテキストで、迅速な学習と機能の再利用について説明します。

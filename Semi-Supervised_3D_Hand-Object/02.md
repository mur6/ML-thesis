2. Related Work
Hand pose estimation. Research in RGB-based hand pose estimation can be generally categorized into two
paradigms, model-free approaches [6, 90, 32, 68, 46, 16] and model-based approaches [3, 5, 36, 88, 24]. Model-free ap- proaches estimate pose by learning joint coordinates [90, 68] or joint heatmaps [32, 46, 6]. For example, Zimmermann et al. [90] proposed to detect 2D hand joints and lift them into 3D with the articulation prior. Differently, model-based approaches utilize the differentiable MANO model [61] to capture hand pose and shape. For example, Boukhayma et al. [5] collected synthetic data for pre-training to increase the hand pose accuracy. In our work, instead of relying on either synthetic data or 3D ground-truths, we leverage the spatial-temporal information in large-scale real-world videos to achieve better hand pose estimation performance and generalization ability in a semi-supervised manner.
Object pose estimation. There are also two main paradigms to perform object 6-Dof pose estimation, with one directly regressing the pose as network outputs [34, 81] and another regressing the projected 3D object control points location in the image and recovering the pose with 2D-to-3D correspondence [55, 73, 53, 29]. Due to the non-linearity of the rotation space, direct regression of the 6-D pose suffers from the generalization problem [29, 79]. For the 2D-to-3D genre, as an example, Hu et al. [29] performed inference by generating pixel-wise structural outputs, containing multiple proposals for computing the pose, which demonstrates strong robustness and efficiency. Our method is inspired by this approach, but further extends to consider the hand-object interaction for object pose estimation. The Transformer architecture in our joint framework performs contextual rea- soning to enhance the object features and lead to better object pose estimation under occlusion.
Hand-object interaction. Simultaneously estimating hand and object poses [72, 11, 49, 50, 23, 12, 9] during interaction is a challenging task due to self-occlusion. To tackle this problem, Hasson et al. [24] leveraged physical constraints for regressing hand and object mesh at the same time using two separate networks. Differently, we observe that sharing the feature backbone in learning between two pose estimation tasks can implicitly encode the context in- formation. And this contextual information becomes very useful when applied to the semi-supervised learning setting. Similar to our approach, Chen et al. [9] proposed to fuse hand-object representations to get interaction-aware features for joint pose estimation using LSTM [19]. However, this feature fusion method cannot model the spatial dependency between hand and object. Instead, our method utilizes a Transformer to perform contextual reasoning between hand- object representations to get interaction-aware feature maps explicitly, while maintaining spatial information. The mod- ule could benefit both hand and object pose estimation.
Semi-supervised Learning. Semi-supervised learning plays a key role in improving model performance when the labeled data is limited [56, 43, 82, 66, 86, 59, 62, 8].
Given a trained model on human-annotated datasets, we can apply it on unlabeled data to collect pseudo-labels for further training [37, 33, 1, 67]. For example, Hinton et al. [27] have proposed to perform model ensembles in testing to improve the estimation performance. Instead of using multiple models, Radosavovic et al. [56] proposed to deploy the trained model with test-time augmentations to increase the confidence of the pseudo-labels. While related to our method, most of the previous approaches have not considered the spatial and temporal constraints in videos for selecting the pseudo-labels, which is one of our innovations.
Interaction Reasoning. Interaction reasoning [75, 64, 77, 74, 76, 78, 63, 4, 28] aims to model the interactions among objects. For example, Santoro et al. [64] inferred relations across all pairs of objects to solve the visual question answering task. Wang et al. [75] captured long- range dependencies via the non-local module in spacetime for video classification. Recent studies show the superior- ity of the Transformer [74] architecture in learning visual relations and using it to solve different computer vision tasks [51, 57, 71, 38, 13, 7, 10], especially for pose esti- mation [84, 44, 70, 30, 40, 21]. The goal of our work is to exploit the visual correlation between hand and object to improve pose estimation performance under occlusion. We adopt the Transformer architecture in the contextual rea- soning module, where we do cross-attention to exploit the relevant pair of cells between hand-object instead of the whole image.

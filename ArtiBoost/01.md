# ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis

## 1. Introduction
Articulated bodies, such as the human hand, body, and linkage mechanism, can be observed every day in our life. Their joints, links, and movable parts depict the functional- ity of the articulation body. Extracting their transient con- figuration from image or video sequence, which is often re- ferred to as Pose Estimation [8, 37, 38], can benefit many downstream tasks in robotics and augment reality. Pose es- timation for multi-body articulations is especially challenging as it suffers from severe self- or mutual occlusion.


In this work, we paid attention to a certain type of multi-body articulations – composited hand and object poses during their interaction [5, 12, 19, 21, 24–26, 30, 32, 43, 69]. Hands are the primary means by which humans manipulate ob- jects in the real-world, and the hand-object pose estimation (HOPE) task holds great potential for understanding human behavior [14, 18, 36, 46, 61].

As the degrees of freedom (DoF) grows, the proper amount of data to cover the pose distribution has grown ex- ponentially. More than the most common articulation bod- ies, the human hand has 16 joints and approximately 21 DoF. Preparing such diverse training data for the HOPE task can be very challenging. The real-world recording and an- notation methods [3,6,14,21] tend to hinder the pose diver- sity. For example, the multi-view-based approaches [3, 21] require the subject to maintain a static grasping pose in a video sequence. As a result, their recording process is in- efficient, and their pose diversity is insufficient. In con- trast, data synthesis is efficient and annotation-free, and has been widely adopted in single-body N-D pose estima- tion [9, 15, 42, 63, 65, 72]. However, these methods are in- eligible for multi-body articulation, in which the poses are restricted by mutual contact and obstruction. Data synthesis for HOPE tasks requires us to simulate virtual hand-object interaction (grasp) that mimics the underlying pose distribu- tion of their real-world counterparts. Conventional method either manually articulated [10, 51, 52] the hand model for grasp, or relegated [2, 26, 34] grasp synthesis to an off-the-shelf grasping simulator: GraspIt [48]. However, the man- ual methods are difficult to scale their data in large amounts, and the simulation method also sacrificed the diversity of hand poses. GraspIt optimizes for hand-crafted grasp met- rics [13] that do not reflect the pose distribution of a 21-DoF dexterous hand. Besides, even with a vast amount of syn- thetic grasps, not every hand-object configuration is help- ful for training. For example, similar configurations may have already been observed multiple times, and those eas- ily discernable samples may have a frequent appearance. Hence, offline data synthesis, without repeatedly commu- nicating with the model during training, is still considered inefficient for a learning task.

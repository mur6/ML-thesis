2. Related Work
Hand pose estimation. Research in RGB-based hand pose estimation can be generally categorized into two paradigms, model-free approaches [6, 90, 32, 68, 46, 16] and model-based approaches [3, 5, 36, 88, 24].
手のポーズ推定。 RGBベースの手のポーズ推定の研究は、一般に2つのパラダイム、モデルフリーアプローチ[6、90、32、68、46、16]とモデルベースアプローチ[3、5、36、88、24]に分類できます。

Model-free approaches estimate pose by learning joint coordinates [90, 68] or joint heatmaps [32, 46, 6]. For example, Zimmermann et al. [90] proposed to detect 2D hand joints and lift them into 3D with the articulation prior. Differently, model-based approaches utilize the differentiable MANO model [61] to capture hand pose and shape. For example, Boukhayma et al. [5] collected synthetic data for pre-training to increase the hand pose accuracy. In our work, instead of relying on either synthetic data or 3D ground-truths, we leverage the spatial-temporal information in large-scale real-world videos to achieve better hand pose estimation performance and generalization ability in a semi-supervised manner.
モデルフリーアプローチは、関節座標[90、68]または関節ヒートマップ[32、46、6]を学習することによってポーズを推定します。たとえば、Zimmermannetal。 [90]は、2Dの手関節を検出し、事前に関節を使って3Dに持ち上げることを提案しました。これとは異なり、モデルベースのアプローチでは、微分可能なMANOモデル[61]を使用して、手のポーズと形状をキャプチャします。たとえば、Boukhaymaetal。 [5]手のポーズの精度を高めるために、事前トレーニングのために合成データを収集しました。私たちの仕事では、合成データや3Dグラウンドトゥルースに依存する代わりに、大規模な実世界のビデオの時空間情報を活用して、半教師ありの方法でより優れた手のポーズ推定パフォーマンスと一般化能力を実現します。

Object pose estimation. There are also two main paradigms to perform object 6-Dof pose estimation, with one directly regressing the pose as network outputs [34, 81] and another regressing the projected 3D object control points location in the image and recovering the pose with 2D-to-3D correspondence [55, 73, 53, 29].
オブジェクトポーズ推定。オブジェクト6-Dofポーズ推定を実行するための2つの主要なパラダイムもあります。1つはネットワーク出力としてポーズを直接回帰し[34、81]、もう1つは画像内の投影された3Dオブジェクトコントロールポイントの位置を回帰し、2D-to-3D対応でポーズを復元します[55、73、53、29]。

Due to the non-linearity of the rotation space, direct regression of the 6-D pose suffers from the generalization problem [29, 79].
回転空間の非線形性のために、6-Dポーズの直接回帰は一般化の問題に悩まされます[29、79]。

For the 2D-to-3D genre, as an example, Hu et al. [29] performed inference by generating pixel-wise structural outputs, containing multiple proposals for computing the pose, which demonstrates strong robustness and efficiency. 
例として、2Dから3Dのジャンルについては、Huetal。 [29]は、ポーズを計算するための複数の提案を含むピクセル単位の構造出力を生成することによって推論を実行しました。

Our method is inspired by this approach, but further extends to consider the hand-object interaction for object pose estimation. The Transformer architecture in our joint framework performs contextual rea- soning to enhance the object features and lead to better object pose estimation under occlusion.
これは、強力な堅牢性と効率性を示しています。私たちの方法はこのアプローチに触発されていますが、オブジェクトのポーズ推定のために手とオブジェクトの相互作用を考慮するようにさらに拡張されています。共同フレームワークのTransformerアーキテクチャは、コンテキスト推論を実行してオブジェクトの機能を強化し、オクルージョン下でのオブジェクトのポーズ推定を改善します。

Hand-object interaction. Simultaneously estimating hand and object poses [72, 11, 49, 50, 23, 12, 9] during interaction is a challenging task due to self-occlusion. To tackle this problem, Hasson et al. [24] leveraged physical constraints for regressing hand and object mesh at the same time using two separate networks. Differently, we observe that sharing the feature backbone in learning between two pose estimation tasks can implicitly encode the context in- formation. And this contextual information becomes very useful when applied to the semi-supervised learning setting. Similar to our approach, Chen et al. [9] proposed to fuse hand-object representations to get interaction-aware features for joint pose estimation using LSTM [19].
手とオブジェクトの相互作用。インタラクション中に手とオブジェクトのポーズ[72、11、49、50、23、12、9]を同時に推定することは、自己閉塞のために困難な作業です。この問題に取り組むために、ハッソン等。 [24] 2つの別々のネットワークを使用して、手とオブジェクトのメッシュを同時に後退させるために物理的な制約を活用しました。これとは異なり、2つのポーズ推定タスク間で学習する際に機能バックボーンを共有すると、コンテキスト情報を暗黙的にエンコードできることがわかります。そして、このコンテキスト情報は、半教師あり学習の設定に適用すると非常に役立ちます。私たちのアプローチと同様に、Chenetal。 [9]は、手オブジェクト表現を融合して、LSTM[19]を使用した関節ポーズ推定のための相互作用認識機能を取得することを提案しました。

However, this feature fusion method cannot model the spatial dependency between hand and object. Instead, our method utilizes a Transformer to perform contextual reasoning between hand- object representations to get interaction-aware feature maps explicitly, while maintaining spatial information. The mod- ule could benefit both hand and object pose estimation.
ただし、この機能融合方法では、手とオブジェクトの間の空間依存性をモデル化できません。代わりに、私たちの方法は、トランスフォーマーを利用して、手オブジェクト表現間でコンテキスト推論を実行し、空間情報を維持しながら、相互作用を意識した機能マップを明示的に取得します。このモジュールは、手と物体の両方のポーズ推定に役立ちます。

Semi-supervised Learning. Semi-supervised learning plays a key role in improving model performance when the labeled data is limited [56, 43, 82, 66, 86, 59, 62, 8].
半教師あり学習。半教師あり学習は、ラベル付けされたデータが制限されている場合にモデルのパフォーマンスを向上させる上で重要な役割を果たします[56、43、82、66、86、59、62、8]。

Given a trained model on human-annotated datasets, we can apply it on unlabeled data to collect pseudo-labels for further training [37, 33, 1, 67]. For example, Hinton et al. [27] have proposed to perform model ensembles in testing to improve the estimation performance. Instead of using multiple models, Radosavovic et al. [56] proposed to deploy the trained model with test-time augmentations to increase the confidence of the pseudo-labels. While related to our method, most of the previous approaches have not considered the spatial and temporal constraints in videos for selecting the pseudo-labels, which is one of our innovations.

Interaction Reasoning. Interaction reasoning [75, 64, 77, 74, 76, 78, 63, 4, 28] aims to model the interactions among objects. For example, Santoro et al. [64] inferred relations across all pairs of objects to solve the visual question answering task. Wang et al. [75] captured long- range dependencies via the non-local module in spacetime for video classification. Recent studies show the superior- ity of the Transformer [74] architecture in learning visual relations and using it to solve different computer vision tasks [51, 57, 71, 38, 13, 7, 10], especially for pose esti- mation [84, 44, 70, 30, 40, 21]. The goal of our work is to exploit the visual correlation between hand and object to improve pose estimation performance under occlusion. We adopt the Transformer architecture in the contextual rea- soning module, where we do cross-attention to exploit the relevant pair of cells between hand-object instead of the whole image.

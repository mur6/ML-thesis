3. Overview
Our method on 3D hand and object pose estimation contains two main components:
3D手とオブジェクトのポーズ推定に関する私たちの方法には、2つの主要なコンポーネントが含まれています。

 (i) a joint learning framework with contextual reasoning between the hand and the object;
 (ii) a semi-supervised pipeline which explores extra labels in videos for training.
 （i）手とオブジェクトの間のコンテキスト推論を備えた共同学習フレームワーク。
 （ii）トレーニング用のビデオ内の追加のラベルを探索する半教師ありパイプライン。

First, we present the hand-object joint learning framework in Section 4.
最初に、セクション4で手とオブジェクトの共同学習フレームワークを示します。
The model contains a shared encoder and two separate decoders for hand and object, as well as a transformer-based contextual reasoning module 4.1 to better exploit their relations.
モデルには、手とオブジェクト用の共有エンコーダーと2つの別個のデコーダー、およびそれらの関係をより有効に活用するためのトランスフォーマーベースのコンテキスト推論モジュール4.1が含まれます。
The model is trained under fully- supervised learning.
モデルは、完全な教師あり学習の下でトレーニングされます。

Then, we propose the semi-supervised learning pipeline in Section 5, Constrained by the spatial-temporal consistency, we generate high-quality pseudo-labels of hand on a large- scale video dataset [18] and re-train our model on the union of fully annotated dataset [20] and those confident pseudo- labels.
次に、セクション5で半教師あり学習パイプラインを提案します。時空間の一貫性に制約され、大規模なビデオデータセット[18]で高品質の手の疑似ラベルを生成し、モデルを再トレーニングします。完全に注釈が付けられたデータセット[20]とそれらの信頼できる疑似ラベルの結合。

Because of the diversity in the hand pseudo-labels, the model could both increase the accuracy of hand pose estimation and generalization.
手の疑似ラベルには多様性があるため、モデルは手のポーズの推定と一般化の両方の精度を高めることができます。

With better hand features as context via the contextual reasoning module, the object pose performance of the model could also be improved.
コンテキスト推論モジュールを介したコンテキストとしてのより優れた手の機能により、モデルのオブジェクトポーズのパフォーマンスも向上する可能性があります。

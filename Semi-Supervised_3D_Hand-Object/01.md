1. Introduction
Hands are humans’ primary means of interacting with the physical world. Capturing the 3D pose of the hands and the objects interacted by hands is a crucial step in un- derstanding human actions. It is also the central part for a variety of applications including augmented reality [54, 31], third-person imitation learning [22, 14], and human-machine interaction [69]. While 3D pose estimation on hands and objects have been studied for a long time in computer vision captured with depth cameras [87, 45, 42, 83, 39] or RGB-D sensors [85, 60, 47, 65, 15] in controlled environments, recent research has also achieved encouraging results on pose estimation from a single monocular RGB image [90, 46, 80].
手は、物理的な世界と相互作用する人間の主要な手段です。手と手が相互作用するオブジェクトの3Dポーズをキャプチャすることは、人間の行動を理解する上で重要なステップです。また、拡張現実[54、31]、第三者の模倣学習[22、14]、人間と機械の相互作用[69]などのさまざまなアプリケーションの中心的な部分でもあります。手や物体の3Dポーズ推定は、深度カメラ[87、45、42、83、39]またはRGB-Dセンサー[85、60、47、65、15]でキャプチャされたコンピュータビジョンで長い間研究されてきました。制御された環境では、最近の研究では、単一の単眼RGB画像からのポーズ推定に関する有望な結果も達成されています[90、46、80]。

Despite the efforts, current approaches still highly rely on human annotations for 3D poses, which are extremely diffi- cult to obtain: Researchers have been collecting data with motion capture [61, 17, 89], or aligning mesh models to the real images [24, 36, 20, 5]. Given insufficient annotations for supervised learning, it limits the trained model from gen- eralizing to novel scenes and out-of-domain environments. To enable better estimation performance and generalization ability, we look into video data of hands and objects in the wild, without using the 3D annotations.
努力にもかかわらず、現在のアプローチは依然として3Dポーズの人間の注釈に大きく依存しており、取得するのは非常に困難です。研究者はモーションキャプチャ[61、17、89]でデータを収集したり、メッシュモデルを実際の画像に位置合わせしたりしています[61、17、89]。 24、36、20、5]。教師あり学習には注釈が不十分であるため、トレーニングされたモデルは一般化から新しいシーンやドメイン外の環境に制限されます。より良い推定パフォーマンスと一般化能力を可能にするために、3D注釈を使用せずに、野生の手とオブジェクトのビデオデータを調べます。

Specifically, we propose to exploit hand-object interactions over time. The poses of the hands and objects are usually highly correlated: The 3D pose of the hand when it is grasping the object often indicates the orientation of the object; the object pose also provides constraints on how the hand can approach and interact with the object.
具体的には、時間の経過とともに手とオブジェクトの相互作用を活用することを提案します。通常、手とオブジェクトのポーズには高い相関関係があります。オブジェクトを握っているときの手の3Dポーズは、多くの場合、オブジェクトの方向を示します。オブジェクトのポーズは、手がオブジェクトに近づき、オブジェクトと相互作用する方法にも制約を与えます。

When observing from the videos, the 3D poses for both hands and objects should change smoothly and continuously.
ビデオから観察すると、手とオブジェクトの両方の3Dポーズがスムーズかつ継続的に変化する必要があります。

This continuity provides a cue for selecting coherent and accurate 3D hand and object pose estimation results when human annotations are not available.
この連続性は、人間の注釈が利用できない場合に、コヒーレントで正確な3D手とオブジェクトのポーズ推定結果を選択するための手がかりを提供します。

In this paper, we introduce a semi-supervised learning approach for 3D hand and object pose estimation with videos. We first train a joint model for both 3D hand pose and 6-Dof object pose estimation with supervised learning using fully annotated data. Then we deploy the model for hand pose estimation in large-scale videos without 3D annotations. We collect the estimation results as novel pseudo-labels for self- training. Specifically, to utilize the interaction information between hand and object, we design a unified framework that extracts the representation from the whole input image, and uses RoIAlign [25] to further obtain the object and hand region representations. Building on these representations, we apply two different branches of sub-networks to estimate the 3D poses for hand and object, respectively. We use the Transformer [74] to bridge the two branches for encoding the mutual context between hand and object. The attention mechanism in Transformer could well model the their relations at every pixel location and allow us to take hand-object interaction as an advantage instead of a disadvantage.
この論文では、ビデオを使用した3D手とオブジェクトのポーズ推定のための半教師あり学習アプローチを紹介します。最初に、完全に注釈が付けられたデータを使用した教師あり学習を使用して、3D手のポーズと6自由度のオブジェクトポーズ推定の両方のジョイントモデルをトレーニングします。次に、3Dアノテーションのない大規模なビデオでの手のポーズ推定用のモデルを展開します。推定結果は、セルフトレーニング用の新しい疑似ラベルとして収集されます。具体的には、手と物体の相互作用情報を活用するために、入力画像全体から表現を抽出する統一フレームワークを設計し、RoIAlign[25]を使用して物体と手の領域の表現をさらに取得します。これらの表現に基づいて、サブネットワークの2つの異なるブランチを適用して、それぞれ手とオブジェクトの3Dポーズを推定します。 Transformer [74]を使用して、手とオブジェクトの間の相互コンテキストをエンコードするための2つのブランチをブリッジします。 Transformerのアテンションメカニズムは、すべてのピクセル位置でのそれらの関係を適切にモデル化でき、手とオブジェクトの相互作用をデメリットではなくアドバンテージとして利用できるようになります。

To perform semi-supervised learning with hand-object videos, we deploy our unified model on each frame for pseudo-label generation, as illustrated in Figure 1. Given the 3D hand pose results from our model, we design spatial- temporal consistency constraints to filter unstable and inac- curate estimations. Intuitively, we only keep the results as pseudo-labels if they change continuously over time, which indicates the robustness of the estimation. We then perform self-training with the newly collected data and labels.
手オブジェクトビデオを使用して半教師あり学習を実行するために、図1に示すように、疑似ラベル生成のために各フレームに統合モデルを展開します。モデルからの3D手のポーズの結果を考慮して、時空間一貫性制約を次のように設計します。不安定で不正確な推定をフィルタリングします。直感的には、結果が時間の経過とともに継続的に変化する場合にのみ、結果を疑似ラベルとして保持します。これは、推定の堅牢性を示しています。次に、新しく収集されたデータとラベルを使用してセルフトレーニングを実行します。

We experiment by training the initial model on the HO- 3D dataset [20], and perform semi-supervised learning with the Something-Something video dataset [18]. By learning from the pseudo-labels from large-scale videos using our approach, we achieve a large gain over state-of-the-art ap- proaches in the HO-3D benchmark.

We also show significant improvements in 3D hand pose estimation which generalizes to the out-of-domain datasets including FPHA [15] and Frei- Hand [91] datasets. More surprisingly, even though we only use pseudo-labels for hands, our joint self-training improves the object pose estimation by a large margin (more than 10% in some objects).

Our contributions include: (i) An unified framework for joint 3D hand and object pose estimation; (ii) A semi- supervised learning pipeline which exploits large-scale unlabeled hand-object interaction videos; (iii) Substantial per- formance improvement on hand and object pose estimation, and generalization to out-of-domain data.
HO-3Dデータセット[20]で初期モデルをトレーニングして実験し、Something-Somethingビデオデータセット[18]を使用して半教師あり学習を実行します。私たちのアプローチを使用して大規模なビデオからの疑似ラベルから学習することにより、HO-3Dベンチマークの最先端のアプローチを大幅に上回ります。また、FPHA[15]およびFrei-Hand[91]データセットを含むドメイン外データセットに一般化する3D手のポーズ推定の大幅な改善も示しています。さらに驚くべきことに、手には疑似ラベルのみを使用していますが、共同セルフトレーニングにより、オブジェクトのポーズ推定が大幅に向上します（一部のオブジェクトでは10％以上）。
私たちの貢献は次のとおりです。（i）3D手とオブジェクトの共同ポーズ推定のための統一されたフレームワーク。 （ii）大規模なラベルのない手と物体の相互作用ビデオを活用する半教師あり学習パイプライン。 （iii）手と物体のポーズ推定の大幅なパフォーマンスの向上、およびドメイン外データへの一般化。


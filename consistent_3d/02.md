2 RELATED WORK

Hand Pose and Shape Estimation. Researchers have devel- oped a lot of different methods in hand pose and shape estimation, such as regression-based method [20], [28], [29], [30], [31] and model-based method [32], [33], [34], [35]. Com- paring to hand pose which is represented by 3D coordinates of hand joints alone, hand mesh contains more detailed shape information and recently has become the focus in the research community. Several methods utilize the hand mesh topology to directly output 3D mesh vertices. E.g. [36], [37], [38] use the spiral convolution to recover hand mesh and [30], [39], [40] use the graph convolution to output mesh vertices. Although these methods introduce as few priors as possible, they require large amounts of annotated data for training. In this self-supervised work, we make use of the priors contained in the MANO hand model [41], where MANO can map pose and shape parameters to a triangle mesh [24], [42], [43], [44], to reduce reliance on the labeled training data.
手のポーズと形状の推定。研究者は、回帰ベースの方法[20]、[28]、[29]、[30]、[31]やモデルベースの方法[32]など、手のポーズや形状の推定においてさまざまな方法を開発してきました。 、[33]、[34]、[35]。手関節だけの3D座標で表される手のポーズと比較すると、ハンドメッシュにはより詳細な形状情報が含まれており、最近では研究コミュニティで注目を集めています。いくつかの方法では、ハンドメッシュトポロジを利用して3Dメッシュ頂点を直接出力します。例えば。 [36]、[37]、[38]はスパイラル畳み込みを使用してハンドメッシュを復元し、[30]、[39]、[40]はグラフ畳み込みを使用してメッシュ頂点を出力します。これらの方法は、可能な限り少ない事前情報を導入しますが、トレーニングのために大量の注釈付きデータを必要とします。この自己監視作業では、MANOハンドモデル[41]に含まれる事前情報を利用します。ここで、MANOはポーズと形状のパラメーターを三角形メッシュにマッピングできます[24]、[42]、[43]、[44]。 、ラベル付けされたトレーニングデータへの依存を減らすため。

Because the parametric model contains abundant struc- ture priors of human hands, recent works integrate hand model as a differentiable layer in neural networks [23], [24], [42], [44], [45], [46], [47], [48]. Among them, [45], [47], [48] output a set of intermediate estimations, like segmentation mask and 2D keypoints, and then map these representations to the MANO parameters. Different from them, we aim at demonstrating the feasibility of a self-supervised framework using an intuitive autoencoder. We additionally output 2D keypoint estimation from another branch and utilize it only during training to facilitate 3D reconstruction. More gener- ally, recent methods [23], [24], [42], [44], [46] directly adopt an autoencoder that couples an image feature encoding stage with a model-based decoding stage. Unlike [23], [24], we focus on hand recovery and do not use any annotation about objects. More importantly, the above methods use 3D annotations as supervision, while the proposed method does not rely on any ground truth annotations.
パラメトリックモデルには人間の手の構造の優先順位が豊富に含まれているため、最近の研究では、ハンドモデルをニューラルネットワークの微分可能層として統合しています[23]、[24]、[42]、[44]、[45]、[46]、 [47]、[48]。その中で、[45]、[47]、[48]は、セグメンテーションマスクや2Dキーポイントなどの一連の中間推定を出力し、これらの表現をMANOパラメーターにマッピングします。それらとは異なり、直感的なオートエンコーダーを使用して、自己監視型フレームワークの実現可能性を実証することを目的としています。さらに、別のブランチから2Dキーポイント推定を出力し、トレーニング中にのみ使用して3D再構築を容易にします。より一般的には、最近の方法[23]、[24]、[42]、[44]、[46]は、画像特徴符号化ステージをモデルベースの復号化ステージと結合するオートエンコーダを直接採用しています。 [23]、[24]とは異なり、手の回復に重点を置いており、オブジェクトに関する注釈は使用していません。さらに重要なことに、上記の方法は監視として3D注釈を使用しますが、提案された方法はグラウンドトゥルース注釈に依存しません。

3D Hand Pose and Shape Estimation with Limited Supervision. 2D annotation is cheaper than 3D annotation, but it is difficult to deal with the ambiguity of depth and scale. [17] uses a depth map to perform additional weak supervision to strengthen 2D supervision. [19] proposes the biomechanical constraints to help the network output feasible 3D hand configurations. [49] detects 2D hand key- points and directly fits a hand model to the 2D detection. [22] gathers a large-scale dataset through an automated data collection method similar to [49] and then applies the collected mesh as supervision. In this work, we limit biomechanical feasibility by introducing a set of constraints on the skin model instead of only imposing constraints on the skeleton as [19]. In contrast to [17], [22], our method is designed to verify the feasibility of (noisy) 2D supervision and avoids introducing any extra 2.5D or 3D data.
制限された監視による3D手のポーズと形状の推定。 2D注釈は3D注釈よりも安価ですが、奥行きとスケールのあいまいさを処理することは困難です。 [17]は、深度マップを使用して追加の弱い監視を実行し、2D監視を強化します。 [19]は、ネットワークが実現可能な3D手の構成を出力するのに役立つ生体力学的制約を提案しています。 [49]は、2D手のキーポイントを検出し、手のモデルを2D検出に直接適合させます。 [22]は、[49]と同様の自動データ収集方法を使用して大規模なデータセットを収集し、収集したメッシュを監視として適用します。この作業では、[19]のように骨格に制約を課すだけでなく、皮膚モデルに一連の制約を導入することにより、生体力学的実現可能性を制限します。 [17]、[22]とは対照的に、私たちの方法は、（ノイズの多い）2D監視の実現可能性を検証するように設計されており、余分な2.5Dまたは3Dデータの導入を回避します。

Self-supervised 3D Reconstruction. Recently, there are methods that propose to learn 3D geometry from the monoc- ular image only. For example, [50] presents an unsupervised approach to learn 3D deformable objects from raw single- view images, but they assume the object is perfectly sym- metric, which is not the case in the hand reconstruction. [51] removes out keypoints from the supervision signals, but it uses ground truth 2D silhouette as supervision and only tackles categories with small intra-class shape differences, such as birds, shoes, and cars. [52] explores a depth-based self-supervised 3D hand pose estimation method, but the depth image provides much more strong evidence and supervision than the RGB image. Recently, [53], [54], [55] exploits a self-supervised face reconstruction method with the usage of 3D morphable model of face (3DMM) [56] and 2D landmarks detection. Our approach is similar to them, but the hand is relatively non-flat and asymmetrical when compared with the 3D face, and the hand suffers from more severe self-occlusion. These characteristics make this self- supervised hand reconstruction task more challenging.

Texture Modeling in Hand Recovery. [57], [58] exploit shading and texture information to handle the self-occlusion problem in the hand tracking system. Recently, [25] uses principal component analysis (PCA) to build a parametric texture model of hand from a set of textured scans. In this work, we try to model texture from self-supervised training without introducing extra data, and further inves- tigate whether the texture modeling helps with the shape modeling.

Motion Learning from Sequence Data for 3D Hand Estimation. To leverage motion information contained in sequence data, several methods have been proposed in hand pose estimation. [23] uses the photometric consistency between neighboring frames of sparsely annotated RGB videos. [59] presents a graph-based method to exploit spatial and temporal relationship for sequence pose estimation. [32], [60] design a temporal consistency loss for motion smoothness. However, these methods either are specialized for motion generation or only impose a weak regularization for motion smoothness. There exists no approach to capture hand motion dynamics fundamentally, leading to limited benefits can be gained from modeling motion. In this work, we aim to exploit self-supervised information from hand motion dynamics. Unlike most of the previous approaches [61], [62], [63], [64] which adopt recurrent or graph-based network structure to learn hand motion in a sequence- to-sequence manner, we instead use a motion-related loss function to help our frame-to-frame model converges better and bridges the gap with fully-supervised methods.

From the above analysis and comparison, we believe that self-supervised 3D hand reconstruction is feasible and significant, but to the best of our knowledge, no such idea has been studied in this field. In this work, we fill this gap and propose the first self-supervised 3D hand reconstruc- tion model, and prove its effectiveness through extensive experiments.

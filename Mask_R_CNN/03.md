# Mask R-CNN
## 3. Mask R-CNN
Mask R-CNN is conceptually simple:
マスクR-CNNは概念的に単純です。

Faster R-CNN has two outputs for each candidate object, a class label and a bounding-box offset;
より高速なR-CNNには、候補オブジェクトごとに2つの出力、クラスラベルとバウンディングボックスオフセットがあります。

to this we add a third branch that outputs the object mask.
これに、オブジェクトマスクを出力する3番目のブランチを追加します。

Mask R-CNN is thus a natural and in- tuitive idea. But the additional mask output is distinct from the class and box outputs, requiring extraction of much finer spatial layout of an object.
したがって、マスクR-CNNは自然で直感的なアイデアです。ただし、追加のマスク出力はクラスおよびボックス出力とは異なり、オブジェクトのより細かい空間レイアウトを抽出する必要があります。

Next, we introduce the key elements of Mask R-CNN, including pixel-to-pixel alignment, which is the main missing piece of Fast/Faster R-CNN.
次に、Fast / Faster R-CNNの主要な欠落部分であるピクセル間アライメントを含む、MaskR-CNNの主要な要素を紹介します。

Faster R-CNN: We begin by briefly reviewing the Faster R-CNN detector [36].
Faster R-CNN：まず、FasterR-CNN検出器を簡単に確認します[36]。

Faster R-CNN consists of two stages.
FasterR-CNNは2つのステージで構成されています。

The first stage, called a Region Proposal Network (RPN), proposes candidate object bounding boxes.
リージョンプロポーザルネットワーク（RPN）と呼ばれる最初のステージでは、候補オブジェクトの境界ボックスを提案します。

The second stage, which is in essence Fast R-CNN [12], extracts fea- tures using RoIPool from each candidate box and performs classification and bounding-box regression.
本質的にFastR-CNN[12]である、第2段階では、各候補ボックスからRoIPoolを使用して特徴を抽出し、分類とバウンディングボックス回帰を実行します。

The features used by both stages can be shared for faster inference. We refer readers to [21] for latest, comprehensive comparisons between Faster R-CNN and other frameworks.
両方のステージで使用される機能を共有して、推論を高速化できます。 Faster R-CNNと他のフレームワークとの最新の包括的な比較については、読者に[21]を参照してください。

Mask R-CNN: Mask R-CNN adopts the same two-stage procedure, with an identical first stage (which is RPN). In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI. This is in contrast to most recent systems, where clas- sification depends on mask predictions (e.g. [33, 10, 26]). Our approach follows the spirit of Fast R-CNN [12] that applies bounding-box classification and regression in par- allel (which turned out to largely simplify the multi-stage pipeline of original R-CNN [13]).
マスクR-CNN：マスクR-CNNは、同じ2段階の手順を採用し、最初の段階は同じです（RPN）。第2段階では、クラスとボックスオフセットの予測と並行して、MaskR-CNNは各RoIのバイナリマスクも出力します。これは、分類がマスクの予測に依存する最新のシステムとは対照的です（[33、10、26]など）。私たちのアプローチは、バウンディングボックスの分類と回帰を並列に適用するFast R-CNN [12]の精神に従います（これにより、元のR-CNN [13]の多段階パイプラインが大幅に簡素化されました）。

Formally, during training, we define a multi-task loss on each sampled RoI as L = Lcls + Lbox + Lmask.
正式には、トレーニング中に、サンプリングされた各RoIのマルチタスク損失をL = Lcls + Lbox+Lmaskとして定義します。

The classification loss Lcls and bounding-box loss Lbox are identical as those defined in [12].
分類損失Lclsとバウンディングボックス損失Lboxは、[12]で定義されているものと同じです。

The mask branch has a Km2- dimensional output for each RoI, which encodes K binary masks of resolution m × m, one for each of the K classes.
マスクブランチには、RoIごとにKm2次元の出力があり、Kクラスごとに1つずつ、解像度m×mのK個のバイナリマスクをエンコードします。

To this we apply a per-pixel sigmoid, and define Lmask as the average binary cross-entropy loss.
これに、ピクセルごとのシグモイドを適用し、Lmaskを平均バイナリクロスエントロピー損失として定義します。

For an RoI associated with ground-truth class k, Lmask is only defined on the k-th mask (other mask outputs do not contribute to the loss).
グラウンドトゥルースクラスkに関連付けられたRoIの場合、Lmaskはk番目のマスクでのみ定義されます（他のマスク出力は損失に寄与しません）。

Our definition of Lmask allows the network to generate masks for every class without competition among classes;
Lmaskの定義により、ネットワークはクラス間で競合することなく、すべてのクラスのマスクを生成できます。

we rely on the dedicated classification branch to predict the class label used to select the output mask. This decouples mask and class prediction. This is different from common practice when applying FCNs [30] to semantic segmenta- tion, which typically uses a per-pixel softmax and a multino- mial cross-entropy loss. In that case, masks across classes compete; in our case, with a per-pixel sigmoid and a binary loss, they do not. We show by experiments that this formu- lation is key for good instance segmentation results.
出力マスクの選択に使用されるクラスラベルを予測するために、専用の分類ブランチに依存しています。これにより、マスクとクラスの予測が分離されます。これは、FCN [30]をセマンティックセグメンテーションに適用する場合の一般的な方法とは異なります。セマンティックセグメンテーションは、通常、ピクセルごとのソフトマックスと多項クロスエントロピー損失を使用します。その場合、クラス間のマスクが競合します。私たちの場合、ピクセルごとのS状結腸と2値の損失があるため、そうではありません。この定式化が優れたインスタンスセグメンテーション結果の鍵であることを実験によって示します。

Mask Representation: A mask encodes an input object’s spatial layout.
マスク表現：マスクは、入力オブジェクトの空間レイアウトをエンコードします。

Thus, unlike class labels or box offsets that are inevitably collapsed into short output vectors by fully-connected (fc) layers, extracting the spatial structure of masks can be addressed naturally by the pixel-to-pixel correspondence provided by convolutions.
したがって、完全に接続された（fc）レイヤーによって必然的に短い出力ベクトルに折りたたまれるクラスラベルやボックスオフセットとは異なり、マスクの空間構造の抽出は、畳み込みによって提供されるピクセル間の対応によって自然に対処できます。

Specifically, we predict an m × m mask from each RoI using an FCN [30]. This allows each layer in the mask branch to maintain the explicit m × m object spatial layout without collapsing it into a vector representation that lacks spatial dimensions.
具体的には、FCNを使用して各RoIからm×mマスクを予測します[30]。これにより、マスクブランチの各レイヤーは、空間次元のないベクトル表現に折りたたむことなく、明示的なm×mオブジェクトの空間レイアウトを維持できます。

Unlike previous methods that re- sort to fc layers for mask prediction [33, 34, 10], our fully convolutional representation requires fewer parameters, and is more accurate as demonstrated by experiments.
マスク予測のためにfcレイヤーに再ソートする以前の方法[33、34、10]とは異なり、完全畳み込み表現では必要なパラメーターが少なく、実験で示されているように正確です。

This pixel-to-pixel behavior requires our RoI features, which themselves are small feature maps, to be well aligned to faithfully preserve the explicit per-pixel spatial corre- spondence. This motivated us to develop the following RoIAlign layer that plays a key role in mask prediction.

RoIAlign: RoIPool [12] is a standard operation for extract- ing a small feature map (e.g., 7×7) from each RoI. RoIPool first quantizes a floating-number RoI to the discrete granu- larity of the feature map, this quantized RoI is then subdi- vided into spatial bins which are themselves quantized, and finally feature values covered by each bin are aggregated (usually by max pooling). Quantization is performed, e.g., on a continuous coordinate x by computing [x/16], where 16 is a feature map stride and [·] is rounding; likewise, quan- tization is performed when dividing into bins (e.g., 7×7). These quantizations introduce misalignments between the RoI and the extracted features. While this may not impact classification, which is robust to small translations, it has a large negative effect on predicting pixel-accurate masks.

To address this, we propose an RoIAlign layer that removes the harsh quantization of RoIPool, properly aligning the extracted features with the input. Our proposed change is simple: we avoid any quantization of the RoI boundaries or bins (i.e., we use x/16 instead of [x/16]). We use bi- linear interpolation [22] to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and aggregate the result (using max or average), see Figure 3 for details. We note that the results are not sensitive to the exact sampling locations, or how many points are sampled, as long as no quantization is performed.

RoIAlign leads to large improvements as we show in §4.2. We also compare to the RoIWarp operation proposed in [10]. Unlike RoIAlign, RoIWarp overlooked the align- ment issue and was implemented in [10] as quantizing RoI just like RoIPool. So even though RoIWarp also adopts bilinear resampling motivated by [22], it performs on par with RoIPool as shown by experiments (more details in Ta- ble 2c), demonstrating the crucial role of alignment.

Network Architecture: To demonstrate the generality of our approach, we instantiate Mask R-CNN with multiple architectures. For clarity, we differentiate between: (i) the convolutional backbone architecture used for feature ex- traction over an entire image, and (ii) the network head for bounding-box recognition (classification and regression) and mask prediction that is applied separately to each RoI.

We denote the backbone architecture using the nomen- clature network-depth-features. We evaluate ResNet [19] and ResNeXt [45] networks of depth 50 or 101 layers. The original implementation of Faster R-CNN with ResNets [19] extracted features from the final convolutional layer of the 4-th stage, which we call C4. This backbone with ResNet-50, for example, is denoted by ResNet-50-C4. This is a common choice used in [19, 10, 21, 39].

We also explore another more effective backbone re- cently proposed by Lin et al. [27], called a Feature Pyra- mid Network (FPN). FPN uses a top-down architecture with lateral connections to build an in-network feature pyramid from a single-scale input. Faster R-CNN with an FPN back- bone extracts RoI features from different levels of the fea- ture pyramid according to their scale, but otherwise the rest of the approach is similar to vanilla ResNet. Using a ResNet-FPN backbone for feature extraction with Mask R- CNN gives excellent gains in both accuracy and speed. For further details on FPN, we refer readers to [27].

For the network head we closely follow architectures presented in previous work to which we add a fully con- volutional mask prediction branch. Specifically, we ex- tend the Faster R-CNN box heads from the ResNet [19] and FPN [27] papers. Details are shown in Figure 4. The head on the ResNet-C4 backbone includes the 5-th stage of ResNet (namely, the 9-layer ‘res5’ [19]), which is compute- intensive. For FPN, the backbone already includes res5 and thus allows for a more efficient head that uses fewer filters.
We note that our mask branches have a straightforward structure. More complex designs have the potential to im- prove performance but are not the focus of this work.
14×14 14×14 ×256 ×80
mask
14×14 14×14 RoI ×256 ×4 ×256
28×28 28×28 ×256 ×80
mask
      4
7×7 7×7 ave RoI ×1024 res5 ×2048
Faster R-CNN w/ ResNet [19]
class 2048 box

Figure 4. Head Architecture: We extend two existing Faster R- CNN heads [19, 27]. Left/Right panels show the heads for the ResNet C4 and FPN backbones, from [19] and [27], respectively, to which a mask branch is added. Numbers denote spatial resolu- tion and channels. Arrows denote either conv, deconv, or fc layers as can be inferred from context (conv preserves spatial dimension while deconv increases it). All convs are 3×3, except the output conv which is 1×1, deconvs are 2×2 with stride 2, and we use ReLU [31] in hidden layers. Left: ‘res5’ denotes ResNet’s fifth stage, which for simplicity we altered so that the first conv oper- ates on a 7×7 RoI with stride 1 (instead of 14×14 / stride 2 as in [19]). Right: ‘×4’ denotes a stack of four consecutive convs.


### 3.1. Implementation Details

We set hyper-parameters following existing Fast/Faster R-CNN work [12, 36, 27]. Although these decisions were made for object detection in original papers [12, 36, 27], we found our instance segmentation system is robust to them.

Training: AsinFastR-CNN,anRoIisconsideredpositive if it has IoU with a ground-truth box of at least 0.5 and negative otherwise. The mask loss Lmask is defined only on positive RoIs. The mask target is the intersection between an RoI and its associated ground-truth mask.

We adopt image-centric training [12]. Images are resized such that their scale (shorter edge) is 800 pixels [27]. Each mini-batch has 2 images per GPU and each image has N sampled RoIs, with a ratio of 1:3 of positive to negatives [12]. N is 64 for the C4 backbone (as in [12, 36]) and 512 for FPN (as in [27]). We train on 8 GPUs (so effective mini- batch size is 16) for 160k iterations, with a learning rate of 0.02 which is decreased by 10 at the 120k iteration. We use a weight decay of 0.0001 and momentum of 0.9. With ResNeXt [45], we train with 1 image per GPU and the same number of iterations, with a starting learning rate of 0.01.

The RPN anchors span 5 scales and 3 aspect ratios, fol- lowing [27]. For convenient ablation, RPN is trained sep- arately and does not share features with Mask R-CNN, un- less specified. For every entry in this paper, RPN and Mask R-CNN have the same backbones and so they are shareable.

Inference: At test time, the proposal number is 300 for the C4 backbone (as in [36]) and 1000 for FPN (as in [27]). We run the box prediction branch on these proposals, followed by non-maximum suppression [14]. The mask branch is then applied to the highest scoring 100 detection boxes. Al- though this differs from the parallel computation used in training, it speeds up inference and improves accuracy (due to the use of fewer, more accurate RoIs). The mask branch can predict K masks per RoI, but we only use the k-th mask, where k is the predicted class by the classification branch. The m×m floating-number mask output is then resized to the RoI size, and binarized at a threshold of 0.5.
Note that since we only compute masks on the top 100 detection boxes, Mask R-CNN adds a small overhead to its Faster R-CNN counterpart (e.g., ∼20% on typical models).


